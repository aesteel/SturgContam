---
title: "Exposure_Analysis3 - Model Selection"
author: "Anna Steel"
date: "9/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readtext) # need this because the ethovision output .txt files have unusual encoding as UTF-16
library(lme4)
#library(rethinking)
library(rprojroot) # only used for find_rstudio_root_file() to set project wd as root
library(patchwork)
library(circular)
library(chron)
library(rstatix)
library(MuMIn)
library(viridis)
library(emmeans)
library(broom)

knitr::opts_knit$set(root.dir = find_rstudio_root_file()) # sets root directory to match project directory, not rmd file location
```

```{r utility functions, include=FALSE}
# calculate hypotenuse of triangle created by two xy locations

distmov = function(x1,x2,y1,y2) { dm = sqrt((x1-x2)^2 + (y1-y2)^2); return(dm)}

# calculate number steps between detections; use summarized dataset (5 pos/sec)
nstep = function(t2,t1, interv=0.2) {
  if (t2-t1 < 0) stop('time steps backwards?')
  nstep <- ( (t2-t1) / interv)
  return(nstep)
  }


circ.mean.na = function(x) {
  x2 = x[!is.na(x)]
  sinr <- sum(sin(x2))
  cosr <- sum(cos(x2))
  circmean <- atan2(sinr, cosr)
  return(circmean)
}


circ.disp.na = function(x) {
  x2 = x[!is.na(x)]
    n <- length(x2)
    c <- sum(cos(x2))
    s <- sum(sin(x2))
    r <- sqrt(c^2 + s^2)
    rbar <- r/n
    var <- 1 - rbar
    data.frame(n, r, rbar, var)
}


unscale = function(x) {
  center = attr(x,"scaled:center")
  scale = attr(x,"scaled:scale")
  if(is.null(center)) {print("This variable is not scaled or does not have mean and sd as attributes"); break}
  unscaled = (x * scale) + center
  attributes(unscaled) <- NULL
  return(unscaled)
}
  

```


## Read in Cleaned Data ("Exposure_Analysis1.Rmd")
```{r read in clean data}
DataSum.raw = read.csv("outputData/Exposure_Outputdata/Bifenthrin_2022_23_Cleaned_Conc_FilteredPrelim_Allhrs.csv")
 # using the one that was filtered coarsely in R (removed many trials that were questionable); revisit this base dataset after editing in ethovision

DataSum.raw$nomconc_ngL = as.numeric(DataSum.raw$nomconc_ngL)
DataSum.raw$temp = as.numeric(DataSum.raw$temp)
DataSum.raw$ExposureHrs = as.numeric(DataSum.raw$ExposureHrs)
DataSum.raw$calcConc = as.numeric(DataSum.raw$calcConc)

```


## Add water temps at end of trial
```{r add indiv water temp increase}
# gstemp = read.csv("//Users/Anna/Documents/ResearchGit/SturgContam/rawData/Exposure_IndivBehaviorVideo_GS2021_Temps.csv")
# wstemp = read.csv("//Users/Anna/Documents/ResearchGit/SturgContam/rawData/Exposure_IndivBehaviorVideo_WS2021_Temps.csv")
# 
# # interpolate missing temps 
# tempdat = rbind(gstemp, wstemp)
# tempdat$time <- times(paste0(tempdat$time, ":00"))
# 
# ggplot(data=tempdat, aes(x=time, y=endtemp)) + geom_point()
# 
# tempdat.redu = tempdat[!(tempdat$Exposure.Hr=="24" & tempdat$species=="WS" & tempdat$Tray %in% c("A","B","C","D")),] # legitimate notes for high temps for A,B, but also outliers for c,d if we're trying to make a high quality line for temp relationship with exposure video times
#  ggplot(data=tempdat.redu, aes(x=time, y=endtemp)) + geom_point(aes(color=species))
# 
# templm = lm(endtemp ~ time + species, data=tempdat.redu)
#  summary(templm) # temp = 6.83281 + 21.94528*time + 0.42187*WS + error
#  plot(templm)
# 
# tempdat$pred.endtemp[tempdat$species=="GS"] = 6.83281 + 21.94528*tempdat$time[tempdat$species=="GS"]
# 
# tempdat$pred.endtemp[tempdat$species=="WS"] = 6.83281 + 21.94528*tempdat$time[tempdat$species=="WS"] + 0.42187
# 
# ggplot(tempdat, aes(x=endtemp, y=pred.endtemp, color=species)) + geom_point() + geom_line(data = data.frame(x=14:20, y=14:20), aes(x=x, y=y), color="black", lty=2)
# 
# DataSum96.T = merge(DataSum96, tempdat, by.x=c("Spp","ExposureHrs","Tray","Replicate"), by.y=c("species","Exposure.Hr","Tray","rep"), all.x=T)
#  # this uses the control as a sample for all of the corresponding replicate numbers

```



# plots and exploration are in Exposure_Analysis2, with select plots copied here



# Statistical Models, predictions, and other posterior queries for each metric (Total distance travelled, Mean velocity, Meander and Turn angle, Use of center zone (% time), Time Active, Full Rotations vs distance traveled) 


## Distance #####
```{r distance data, echo=F}
## Summarize to take total distance (sum moved in a trial) for each fish replicates
DataSum.dist = DataSum.raw %>%
  group_by(spp, ExposureHrs, nomconc_ngL, temp, calcConc, tray, Arena) %>%
  summarize(TotalfishDist_m = sum(SumDistMoved, na.rm=T)/(10*100)) %>%
  ungroup()  %>%
  mutate(lnCalcConc = log(calcConc+1)) %>%
  mutate(lnCalcConc2 = (log(calcConc+1))^2) %>%
  #mutate(lnCalcConcC= scale(lnCalcConc, scale=FALSE)) %>%
  #mutate(lnCalcConcC2 = lnCalcConcC^2 ) %>% 
   # order of transformations matters (log then scale then square)
  data.frame()



```

```{r distance rawdata plots}
## with 96
totfishdist.bif22 = ggplot(DataSum.dist, 
                             aes(fill=factor(nomconc_ngL), y=TotalfishDist_m, 
                               x=factor(ExposureHrs) ) ) + 
                      geom_boxplot(outlier.size = .75) + 
                      facet_grid(spp~temp, labeller = labeller(
                        Spp = c("GS"="Green Sturgeon","WS"="White Sturgeon"),
                        temp = c("12"="12C", "15"="15C", "18"="18C")),
                        scales="free") +
                      ylab("Total Trial Distance (m)") + 
                      xlab("Nominal Bifenthrin Concentration (ng/L)") +
                      scale_fill_manual(values=viridis(6)[1:4], 
                              name="Nominal Bifenthrin\nConcentration (ng/L)") + 
                      theme_bw()

totfishdist.bif22


```


```{r distance frequentist linear models - combined all hours}

DataSum.dist.scaled = DataSum.dist %>%
  mutate(temp.sc = scale(temp)) %>%
  mutate(calcConc.sc = scale(calcConc)) %>% 
  mutate(lnCalcConc.sc = scale(lnCalcConc))

# check linear relationship btwn calc conc and dist
ggplot(DataSum.dist, aes(x=calcConc, y=TotalfishDist_m, color=spp)) + 
  geom_point() + 
  geom_smooth(method="lm") + 
  facet_grid(spp ~ temp) + 
  theme_bw()

ggplot(DataSum.dist, aes(x=lnCalcConc, y=TotalfishDist_m, color=spp)) + 
  geom_point() + 
  geom_smooth(method="lm") + 
  facet_grid(spp ~ temp) + 
  theme_bw()

ggplot(DataSum.dist, aes(x=lnCalcConc, y=log(TotalfishDist_m), color=spp)) + 
  geom_point() + 
  geom_smooth(method="lm") + 
  facet_grid(spp ~ temp) + 
  theme_bw()


### looked at several models/variables, all quite similar 
# disttemp.LMint = lm(TotalfishDist_m ~ spp*calcConc.sc*temp.sc,
#                data=DataSum.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMint) # some issues at the upper tail of the distn, residuals great
#  summary(disttemp.LMint)  # adjRsq = 0.72
#  dredge(disttemp.LMint)  # all selected in best model
# 
# disttemp.LMadd = lm(TotalfishDist_m ~ spp + calcConc.sc + temp.sc,
#                data=DataSum.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMadd) # same issues at the upper tail of the distn, less great residuals
#  summary(disttemp.LMadd)  # adjRsq = 0.71
#  dredge(disttemp.LMadd)  # all selected in best model
# 
# disttemp.LMaddlog = lm(TotalfishDist_m ~ spp + lnCalcConc.sc + temp.sc,
#                data=DataSum.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMaddlog) # doesn't fix issues
#  summary(disttemp.LMaddlog)  #  adjRsq = 0.71
#  dredge(disttemp.LMaddlog)  # all selected in best model
# 
# disttemp.LMaddquad = lm(TotalfishDist_m ~ spp*calcConc*temp + spp*(calcConc^2)*temp,
#                data=DataSum.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMaddquad) # doesn't fix issues
#  summary(disttemp.LMaddquad)  #  adjRsq = 0.72
#  dredge(disttemp.LMaddquad)  # all significant
# 
# disttemp.LMint2 = lm(TotalfishDist_m ~ spp*calcConc.sc + spp*temp.sc + calcConc.sc*temp.sc,
#                data=DataSum.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMint2) # some issues at the upper tail of the distn, residuals great
#  summary(disttemp.LMint2)  #  adjRsq = 0.72
#  dredge(disttemp.LMint2)  # all selected except conc*spp
#  
disttemp.LMint3 = lm(TotalfishDist_m ~ spp*temp.sc + calcConc.sc*temp.sc + tray,
               data=DataSum.dist.scaled, na.action=na.fail) 
 plot(disttemp.LMint3) # some issues at the upper tail of the distn, residuals great
 summary(disttemp.LMint3)  #  adjRsq = 0.72 (0.73 if add tray)
 #dredge(disttemp.LMint3)  # all selected in best model
 
  DataSum.dist.scaled.GS = filter(DataSum.dist.scaled, spp=="GS")
disttemp.LMint3.GS = lm(TotalfishDist_m ~ lnCalcConc.sc*temp.sc + tray,
               data=DataSum.dist.scaled.GS, na.action=na.fail) 
 plot(disttemp.LMint3.GS) # strong issues at the upper tail of the distn, slope to residuals; GS data not great
 summary(disttemp.LMint3.GS)  #  adjRsq = 0.17 (with and without tray)
 #dredge(disttemp.LMint3.GS)  # interaction barely in best model (dAIC is only 0.71)
 
  DataSum.dist.scaled.WS = filter(DataSum.dist.scaled, spp=="WS")
disttemp.LMint3.WS = lm(TotalfishDist_m ~ calcConc.sc*temp.sc + tray,
               data=DataSum.dist.scaled.WS, na.action=na.fail) 
 plot(disttemp.LMint3.WS) # minor issues at the upper tail of the distn, residuals great
 summary(disttemp.LMint3.WS)  # adjRsq = 0.40 (0.3 without tray)
 #dredge(disttemp.LMint3.WS)  # all vars in best model
 
 
 
 
 
## assess mixed models
disttemp.LMMint1 = lmer(TotalfishDist_m ~ calcConc.sc*temp.sc + (1|spp) + (1|tray),
               data=DataSum.dist.scaled, na.action=na.fail) 
     plot(disttemp.LMMint1) # residuals good
     qqnorm(resid(disttemp.LMMint1)); qqline(resid(disttemp.LMMint1)) # pretty good
     summary(disttemp.LMMint1)  # similar to non-mixed model; spp is hugely important, tray is as well (estimates from 0 to 2, while fixed effect sizes range from -1 to 3.4)
     dredge(disttemp.LMMint1) 
 
disttemp.LMMint2 = lmer(TotalfishDist_m ~ spp + calcConc.sc*temp.sc + (1|tray),
               data=DataSum.dist.scaled, na.action=na.fail) 
     plot(disttemp.LMMint2) # residuals good
     qqnorm(resid(disttemp.LMMint2)); qqline(resid(disttemp.LMMint2)) # pretty good
     summary(disttemp.LMMint2)  # similar to non-mixed model; spp is hugely important, tray is as well (estimates from 0 to 2, while fixed effect sizes range from -1 to 3.4)
     dredge(disttemp.LMMint2) 
  #### estimates are nearly identical when spp is a random vs fixed effect. Use LMMint2.
     
     
     
### assess ANOVA models
     
anv1 = aov(lm(TotalfishDist_m ~ spp + factor(nomconc_ngL)*factor(temp), 
              data = DataSum.dist))
summary(anv1)
TukeyHSD(anv1) 
  # model with no interactions:
  # 500 different from all others; all temperatures are different

  # model with interactions, before custom contrasts designed: 
  # 12C: 500 different from all; 15C: 500 different from 0 and 100; 18C: no differences
  # all pairwise comparisons between temps at same concentration were significany, except 100ng/L at 15 and 18C. 
 

   
anvGS = aov(lm(TotalfishDist_m ~ factor(nomconc_ngL)*factor(temp), 
              data = filter(DataSum.dist, spp=="GS") ) )
summary(anvGS)
TukeyHSD(anvGS) # 500 different from 0 and 10, not 100; all temps different
                # interaction less useful, but sig for 18 vs 12 and vs 15 at 0ng/L, and generally also sig for 12C@500ng/L vs most of the 18C concentrations

anvWS = aov(lm(TotalfishDist_m ~ factor(nomconc_ngL)*factor(temp), 
              data = filter(DataSum.dist, spp=="WS") ) )
summary(anvWS)
TukeyHSD(anvWS) # 500 different from 0, 10, 100; all temps different

      
```


```{r distance frequentist linear models @ 96hrs}

DataSum96.dist = filter(DataSum.dist, ExposureHrs==96)
DataSum96.dist$trayspp = paste(DataSum96.dist$spp, DataSum96.dist$tray, sep="-")

DataSum96.dist.scaled = DataSum96.dist %>%
  mutate(temp.sc = scale(temp)) %>%
  mutate(calcConc.sc = scale(calcConc)) %>% 
  mutate(lnCalcConc.sc = scale(lnCalcConc))

# check linear relationship btwn calc conc and dist
ggplot(DataSum96.dist, aes(x=calcConc, y=TotalfishDist_m, color=spp)) + 
  geom_point() + 
  geom_smooth(method="loess") + 
  facet_grid(spp ~ temp) + 
  theme_bw()

ggplot(DataSum96.dist, aes(x=lnCalcConc, y=TotalfishDist_m, color=spp)) + 
  geom_point() + 
  geom_smooth(method="loess") + 
  facet_grid(spp ~ temp) + 
  theme_bw()

ggplot(DataSum96.dist, aes(x=lnCalcConc, y=log(TotalfishDist_m), color=spp)) + 
  geom_point() + 
  geom_smooth(method="loess") + 
  facet_grid(spp ~ temp, scales="free") + 
  theme_bw()

ggplot(DataSum96.dist, aes(x=trayspp, y=TotalfishDist_m, color=spp)) + 
  geom_boxplot() + 
  facet_wrap(~ temp, scales="free") + 
  theme_bw()

### looked at several models/variables, all quite similar 
# disttemp.LMint = lm(TotalfishDist_m ~ spp*calcConc.sc*temp.sc,
#                data=DataSum96.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMint) # some issues at the upper tail of the distn, residuals great
#  summary(disttemp.LMint)  # adjRsq = 0.72
#  #dredge(disttemp.LMint)  # all selected in best model
# 
# disttemp.LMintlog = lm(TotalfishDist_m ~ spp*lnCalcConc.sc*temp.sc,
#                data=DataSum96.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMintlog) # some issues at the upper tail of the distn, residuals great
#  summary(disttemp.LMintlog)  # adjRsq = 0.72
#  #dredge(disttemp.LMint)  # all selected in best model
#  
# disttemp.LMadd = lm(TotalfishDist_m ~ spp + calcConc.sc + temp.sc,
#                data=DataSum96.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMadd) # same issues at the upper tail of the distn, less great residuals
#  summary(disttemp.LMadd)  # adjRsq = 0.71
#  #dredge(disttemp.LMadd)  # all selected in best model
# 
# disttemp.LMaddlog = lm(TotalfishDist_m ~ spp + lnCalcConc.sc + temp.sc,
#                data=DataSum96.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMaddlog) # doesn't fix issues
#  summary(disttemp.LMaddlog)  #  adjRsq = 0.71
#  #dredge(disttemp.LMaddlog)  # all selected in best model
# 
# disttemp.LMintquad = lm(TotalfishDist_m ~ spp*calcConc*temp + spp*(calcConc^2)*temp,
#                data=DataSum96.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMintquad) # doesn't fix issues
#  summary(disttemp.LMintquad)  #  adjRsq = 0.72
#  #dredge(disttemp.LMaddquad)  # all significant
# 
# disttemp.LMint2 = lm(TotalfishDist_m ~ spp*calcConc.sc + spp*temp.sc + calcConc.sc*temp.sc,
#                data=DataSum96.dist.scaled, na.action=na.fail)
#  plot(disttemp.LMint2) # some issues at the upper tail of the distn, residuals great
#  summary(disttemp.LMint2)  #  adjRsq = 0.72
#  # dredge(disttemp.LMint2)  # all selected except conc*spp
# 
# disttemp.LMint3 = lm(TotalfishDist_m ~ spp*temp.sc + calcConc.sc*temp.sc + tray,
#                data=DataSum96.dist.scaled, na.action=na.fail) 
#  plot(disttemp.LMint3) # some issues at the upper tail of the distn, residuals great
#  summary(disttemp.LMint3)  #  adjRsq = 0.72 (0.73 if add tray)
#  #dredge(disttemp.LMint3)  # all selected in best model
#  
#   DataSum96.dist.scaled.GS = filter(DataSum96.dist.scaled, spp=="GS")
# disttemp.LMint3.GS = lm(TotalfishDist_m ~ lnCalcConc.sc*temp.sc + tray,
#                data=DataSum96.dist.scaled.GS, na.action=na.fail) 
#  plot(disttemp.LMint3.GS) # strong issues at the upper tail of the distn, slope to residuals; GS data not great
#  summary(disttemp.LMint3.GS)  #  adjRsq = 0.17 (with and without tray)
#  #dredge(disttemp.LMint3.GS)  # interaction barely in best model (dAIC is only 0.71)
#  
#   DataSum96.dist.scaled.WS = filter(DataSum96.dist.scaled, spp=="WS")
# disttemp.LMint3.WS = lm(TotalfishDist_m ~ calcConc.sc*temp.sc + tray,
#                data=DataSum96.dist.scaled.WS, na.action=na.fail) 
#  plot(disttemp.LMint3.WS) # minor issues at the upper tail of the distn, residuals great
#  summary(disttemp.LMint3.WS)  # adjRsq = 0.40 (0.3 without tray)
#  #dredge(disttemp.LMint3.WS)  # all vars in best model
 
 
 
 
 
## assess mixed models; tray is important
disttemp.LMMint1 = lmer(TotalfishDist_m ~ calcConc.sc*temp.sc + (1|spp) + (1|trayspp),
               data=DataSum96.dist.scaled, na.action=na.fail) 
     plot(disttemp.LMMint1) # residuals good
     qqnorm(resid(disttemp.LMMint1)); qqline(resid(disttemp.LMMint1)) # pretty good
     summary(disttemp.LMMint1)  # similar to non-mixed model; spp is hugely important, tray is as well (estimates from 0 to 2, while fixed effect sizes range from -1 to 3.4)
     dredge(disttemp.LMMint1) 
 
     
     
############# best linear model ##################################################     
disttemp.LMMint2 = lmer(TotalfishDist_m ~ spp + calcConc.sc * temp.sc + (1|trayspp),
               data=DataSum96.dist.scaled, na.action=na.fail) 
     
     plot(disttemp.LMMint2) # residuals good
     qqnorm(resid(disttemp.LMMint2)); qqline(resid(disttemp.LMMint2)) # pretty good
     summary(disttemp.LMMint2)  # similar to non-mixed model; spp is hugely important, tray is as well (estimates from 0 to 2, while fixed effect sizes range from -1 to 3.4)
     dredge(disttemp.LMMint2) 
  #### estimates are nearly identical when spp is a random vs fixed effect. Use LMMint2.
#################################################################################     

     
     
### assess factor models with custom contrasts ######
# this was best when dredged with interaction btwn nomconc and temp   
disttemp.LMMfact2 = lmer(TotalfishDist_m ~ spp+factor(nomconc_ngL)+factor(temp) + (1|tray), data=DataSum96.dist, na.action=na.fail) 
     plot(disttemp.LMMfact2) # residuals good enough
     qqnorm(resid(disttemp.LMMfact2)); qqline(resid(disttemp.LMMfact2)) # pretty good
     anova(disttemp.LMMfact2)


     cC.emm <- emmeans(disttemp.LMMfact2, "nomconc_ngL")
     cC.emm # averaged over spp and temp
       # nomconc_ngL emmean   SE   df lower.CL upper.CL
       #           0   22.3 1.13 36.4     20.0     24.6
       #          10   20.5 1.12 36.7     18.2     22.8
       #         100   20.4 1.12 35.9     18.1     22.6
       #         500   15.5 1.12 36.1     13.2     17.7
    # non-overlapping confedence intervals for 500 and all others
         
         contrast(cC.emm, 'tukey') 
           #     contrast                        estimate   SE  df t.ratio p.value
           # nomconc_ngL0 - nomconc_ngL10       1.761 1.29 250   1.369  0.5202
           # nomconc_ngL0 - nomconc_ngL100      1.926 1.28 249   1.505  0.4362
           # nomconc_ngL0 - nomconc_ngL500      6.834 1.28 250   5.330  <.0001
           # nomconc_ngL10 - nomconc_ngL100     0.165 1.28 249   0.129  0.9992
           # nomconc_ngL10 - nomconc_ngL500     5.072 1.28 249   3.961  0.0006
           # nomconc_ngL100 - nomconc_ngL500    4.907 1.28 249   3.845  0.0009
    
         contrast(cC.emm, 'poly') %>% head(3) 
           # contrast  estimate   SE  df t.ratio p.value
           # linear      -20.67 4.05 250  -5.101  <.0001
           # quadratic    -3.15 1.81 249  -1.737  0.0836
           # cubic        -6.34 4.05 250  -1.564  0.1192
         
        
     t.emm <- emmeans(disttemp.LMMfact2, "temp")
     t.emm # averaged over nomconc and temp
           # temp emmean   SE    df lower.CL upper.CL
           #   12   14.8 1.12 16.81     12.4     17.1
           #   15   18.7 1.37  9.46     15.7     21.8
           #   18   25.4 1.17 18.08     23.0     27.9

         contrast(t.emm, 'tukey') 
           # contrast        estimate   SE    df t.ratio p.value
           # temp12 - temp15    -3.97 1.77  11.7  -2.247  0.1042
           # temp12 - temp18   -10.66 1.15 257.6  -9.287  <.0001
           # temp15 - temp18    -6.69 1.80  12.2  -3.715  0.0076
    
         contrast(t.emm, 'poly') %>% head(3) 
           # contrast  estimate   SE     df t.ratio p.value
           # linear       10.66 1.15 257.55   9.287  <.0001
           # quadratic     2.72 3.38   9.65   0.805  0.4404
         
           
```




## plot frequentist model predictions for linear model
```{r model predictions, eval=FALSE}
              
###### FAIL SO FAR ############    

preddat.temp.response = unique(data.frame(spp=DataSum96.dist.scaled$spp,
                                   #calcConc=unscale(DataSum96.dist.scaled$calcConc.sc),
                                   #temp = unscale(DataSum96.dist.scaled$temp.sc),
                                   nomconc_ngL = DataSum96.dist.scaled$nomconc_ngL,
                                   calcConc.sc = DataSum96.dist.scaled$calcConc.sc,
                                   temp.sc = DataSum96.dist.scaled$temp.sc,
                                   trayspp = "Z") )
preddat.temp.response$predict = predict(disttemp.LMMint2, newdata=preddat.temp.response, 
                               se.fit=FALSE, allow.new.levels=TRUE)

# 
#  # plot model predictions with raw conc scale
# ggplot(preddat.temp.response, aes(x=calcConc, y=predict, group=temp, fill=spp)) +
#   # geom_errorbar(aes(ymin = predTotalDist_m - predSE_TotalDist_m, 
#   #                   ymax = predTotalDist_m + predSE_TotalDist_m)) + 
#   geom_line(size=0.7) + 
#   geom_point(pch=21, size=3) + 
#   #scale_fill_manual(values=c("GS"="green3","WS"="grey80"),
#                     # labels=c("Green Sturgeon","White Sturgeon"),
#                     # name="Species") + 
#   scale_y_continuous(name="Total Distance Traversed (cm)", 
#                      breaks=seq(0,40,5), 
#                      labels=seq(0,40,5)*100, 
#                      limits=c(0,43)) + 
#   facet_grid(spp~temp)+
#   xlab("Bifenthrin Concentration (ng/L)") +
#   theme_bw()



preds = ggplot(preddat.temp.response, aes(x=factor(nomconc_ngL), y=predict, group=factor(nomconc_ngL), fill=factor(temp))) +
  geom_point(pch=21, size=3) + 
  #scale_fill_manual(values=c("GS"="green3","WS"="grey80"),
                    # labels=c("Green Sturgeon","White Sturgeon"),
                    # name="Species") + 
  scale_y_continuous(name="Total Distance Traversed (cm)", 
                     breaks=seq(0,40,5), 
                     labels=seq(0,40,5)*100, 
                     limits=c(0,43)) + 
  facet_wrap(~spp)+
  xlab("Bifenthrin Concentration (ng/L)") +
  theme_bw()

preds + geom_boxplot(data = DataSum96.dist, aes(y=TotalfishDist_m), outlier.size = .75) +
   geom_point(pch=21, size=3) 
 






tiff("figures/Exposure_OutputFigs/TotalDistance_Bif2022_WS_withPreds.tiff", width=140, height=70, units="mm", compression="lzw", res=150)
totfishdist96.bif22
dev.off()        


# 
#        
# # are the estimates different from control?      
#  summary(dist.lmGS.redu)
#   summary(aov(dist.lmGS.redu))
#  # none of the concentrations are significantly different for GS
#  summary(dist.lmWS.redu)
#   summary(aov(dist.lmWS.redu))
#   trash = lm(TotalfishDist_m ~ factor(lnCalcConc) + factor(lnCalcConc2) + pred.endtemp,
#                data=DataSum96.distWS[DataSum96.distWS$Treatment %in% c(0,1,10,100),]) 
#     summary(trash) 
#     plot(trash)
#       TukeyHSD(aov(trash))
#   # if this is a legit way to model it, the 100 is sig dif from 0. 

```



## central zone

```{r center zone frequentist linear models @ 96hrs}
# thrown together hastily; review

DataSum.cent = DataSum.raw %>%
  filter(!is.na(InZone_C)) %>%  
  group_by(spp, ExposureHrs, Arena, tray, nomconc_ngL, calcConc, temp) %>%
  summarize(InZoneC = sum(InZone_C, na.rm=T), npos = n()) %>%
  ungroup()  %>%
  mutate(PercZoneC.1 = InZoneC / npos) %>%
  data.frame()

DataSum96.cent = filter(DataSum.cent, ExposureHrs==96)
DataSum96.cent$trayspp = paste(DataSum96.cent$spp, DataSum96.cent$tray, sep="-")

DataSum96.cent.scaled = DataSum96.cent %>%
  mutate(temp.sc = scale(temp)) %>%
  mutate(calcConc.sc = scale(calcConc))


 
 ##### left off here, and in the red text on page 14 in the report


############# best linear model ##################################################    
 ## assess mixed models; tray is important
centtemp.LMMint1 = glmer(cbind(InZoneC, (npos-InZoneC)) ~ spp*calcConc.sc + calcConc.sc*temp.sc + (1|trayspp),data=DataSum96.cent.scaled, family="binomial", na.action=na.fail) 
     plot(centtemp.LMMint1) # residuals good
     qqnorm(resid(centtemp.LMMint1)); qqline(resid(centtemp.LMMint1)) # pretty good
     summary(centtemp.LMMint1)  # similar to non-mixed model; spp is hugely important, tray is as well (estimates from 0 to 2, while fixed effect sizes range from -1 to 3.4)
     dredge(centtemp.LMMint1) 
 
centtemp.LMMadd1 = glmer(cbind(InZoneC, (npos-InZoneC)) ~ spp +calcConc.sc + temp.sc + (1|trayspp),data=DataSum96.cent.scaled, family="binomial", na.action=na.fail) 
     plot(centtemp.LMMadd1) # residuals good
     qqnorm(resid(centtemp.LMMadd1)); qqline(resid(centtemp.LMMadd1)) # pretty good
     summary(centtemp.LMMadd1)  # similar to non-mixed model; spp is hugely important, tray is as well (estimates from 0 to 2, while fixed effect sizes range from -1 to 3.4)
     dredge(centtemp.LMMadd1) 
      
     
     
     
### assess factor models with custom contrasts ######
# this was best when dredged with interaction btwn nomconc and temp   
centtemp.LMMfact2 = glmer(cbind(InZoneC, (npos-InZoneC)) ~ spp+factor(nomconc_ngL)+factor(temp) + (1|tray), data=DataSum96.cent.scaled, na.action=na.fail, family="binomial") 
     plot(centtemp.LMMfact2) # residuals good enough
     qqnorm(resid(centtemp.LMMfact2)); qqline(resid(centtemp.LMMfact2)) # pretty good
     anova(centtemp.LMMfact2)


     cC.emm <- emmeans(centtemp.LMMfact2, "nomconc_ngL")
     cC.emm # averaged over spp and temp
     # nomconc_ngL emmean    SE  df asymp.LCL asymp.UCL
     #           0  -2.63 0.107 Inf     -2.84     -2.42
     #          10  -2.63 0.107 Inf     -2.84     -2.42
     #         100  -2.40 0.107 Inf     -2.61     -2.19
     #         500  -1.93 0.107 Inf     -2.14     -1.72
     # non-overlapping confedence intervals for 500 and all others
         
         contrast(cC.emm, 'tukey') 
     # contrast                        estimate      SE  df z.ratio p.value
     # nomconc_ngL0 - nomconc_ngL10    -0.00252 0.00937 Inf  -0.269  0.9932
     # nomconc_ngL0 - nomconc_ngL100   -0.23824 0.00886 Inf -26.902  <.0001
     # nomconc_ngL0 - nomconc_ngL500   -0.70608 0.00826 Inf -85.435  <.0001
     # nomconc_ngL10 - nomconc_ngL100  -0.23571 0.00883 Inf -26.690  <.0001
     # nomconc_ngL10 - nomconc_ngL500  -0.70356 0.00821 Inf -85.660  <.0001
     # nomconc_ngL100 - nomconc_ngL500 -0.46784 0.00765 Inf -61.150  <.0001
         # they are all different except for o and 10...not sure this is true?
    
         contrast(cC.emm, 'poly') %>% head(3) 
         # contrast  estimate     SE  df z.ratio p.value
         # linear     2.35395 0.0263 Inf  89.514  <.0001
         # quadratic  0.46532 0.0121 Inf  38.528  <.0001
         # cubic     -0.00106 0.0278 Inf  -0.038  0.9694
         # should consider the quadratic fit in the future
         
        
     t.emm <- emmeans(centtemp.LMMfact2, "temp")
     t.emm # averaged over nomconc and temp
         # temp emmean    SE  df asymp.LCL asymp.UCL
         #   12  -1.91 0.131 Inf     -2.17     -1.66
         #   15  -2.51 0.185 Inf     -2.87     -2.14
         #   18  -2.77 0.131 Inf     -3.03     -2.52

         contrast(t.emm, 'tukey') 
         # contrast        estimate      SE  df z.ratio p.value
         # temp12 - temp15    0.591 0.22624 Inf   2.613  0.0244
         # temp12 - temp18    0.857 0.00814 Inf 105.245  <.0001
         # temp15 - temp18    0.266 0.22630 Inf   1.175  0.4681
         # 12 dif from 15 and 18 (p<0.03)
    
         contrast(t.emm, 'poly') %>% head(3) 
         # contrast  estimate      SE  df  z.ratio p.value
         # linear      -0.857 0.00814 Inf -105.245  <.0001
         # quadratic    0.325 0.45247 Inf    0.719  0.4723
         
           
```



#### left of here after a hasty run through a variety of stats and an unsatisfying interpretation for the final report. Ugg. 
 
     
















## Velocity Models - abandoned b/c very similar pattern to distance #####
### Velocity 
```{r velocity plots, echo=F}

## Summarize movement velocity
 DataSum9696.vel = DataSum2496.raw %>%
  group_by(index, Trial, Arena, Replicate, ExposureHrs, Treatment, calcConc, Spp, RepID) %>%
  summarize(MnVel = mean(MnVel, na.rm=T), npos = n()) %>%
  ungroup()  %>%
  data.frame()
```



```{r velocity models, echo=F}
vel.lm = lm(MnVel ~ factor(Spp)+ factor(Treatment)*factor(ExposureHrs) , data = DataSum2496.vel)
velspp.lm = lm(MnVel ~ factor(Spp)*factor(Treatment)*factor(ExposureHrs) , data = DataSum2496.vel)

velspp.lmm = lmer(MnVel ~ factor(Spp)*factor(Treatment)*factor(ExposureHrs)+ (1| RepID), data = DataSum2496.vel)

 plot(velspp.lmm)
 qqnorm(resid(velspp.lmm)); qqline(resid(velspp.lmm))
 # plots look okay; the tree-way interaction makes for a better distn of residuals
 
 summary(velspp.lm) # everything is significant except for expsure hrs when modeled with log(conc)
   # effect of species is vastly stronger than anything else
   
   TukeyHSD(aov(velspp.lm))[[4]]
    # for WS, 2000 vs all other treatments are sig diff, and only one other (1000-100)
    # for GS, no GS-GS concentrations were significantly different in this model
   
   TukeyHSD(aov(velspp.lm))[[5]]
    # Ws-GS different at 24 and 69 hours, and WS-WS / GS-GS different at 24 & 96 hours; should do custom contrasts to publish because this is making all possible contrasts. Acutally, should fit in baysian model to publish and compare poterior predicted distribitions to compare. 
  
   TukeyHSD(aov(velspp.lm))[[6]]
    ## if use the three-way interaction model, identify custom contrasts of interest. Thus applies to frequentist of bayesian analysis

```





## Meander Models  #####
```{r meander data, echo=F}
## Add transformations of concentrations, exposure hours, and meander
 
DataSum2496 <- DataSum2496.raw %>%
  mutate(MeanderCircLog = log(abs(MeanderCirc)+1)) %>%
  mutate(lnCalcConc = log(calcConc+1)) %>%
  mutate(lnCalcConc2 = lnCalcConc^2) %>%
  mutate(lnCalcConcC= as.numeric(scale(lnCalcConc, scale=FALSE))) %>%
  mutate(lnCalcConcC2 = lnCalcConcC ^ 2 ) %>% 
   # order of transformations matters (log then scale then square)
  data.frame()

## Summarize meander and turn angle

# trying this to see if the tiny are causing the problems - could reasonably be argued that these are within the error range of the computer tracking
DataSum2496[!is.na(DataSum2496$MeanderCirc) & DataSum2496$MeanderCirc<1,"MeanderCirc"] <- NA

## Summarize meander and turn angle
DataSum2496.meand = DataSum2496 %>%
  group_by(index, Trial, Arena, Replicate, ExposureHrs, Treatment, 
           calcConc, lnCalcConc, lnCalcConc2, lnCalcConcC, lnCalcConcC2, Spp, RepID) %>%
  summarize(MnTurnAngle = deg(circ.mean.na(rad(TurnAngleCirc))), 
            VarTurnAngle = deg(circ.disp.na(rad(TurnAngleCirc))$var), # circular variance
            MnMeander = mean(MeanderCirc, na.rm=T),
            SDMeander = sd(MeanderCirc, na.rm=T))  %>%
  ungroup()  %>%
  data.frame()



#keep raw data but remove missing points
#DataSum2496.mraw = DataSum2496[!is.na(DataSum2496$TurnAngle_deg),]
# MeanMeand0 = DataSum2496.meand %>%
#   subset(Treatment==0) %>%
#   group_by(Spp, ExposureHrs) %>%
#   summarize(mean0_meand = mean(MnMeander)) %>%
#   ungroup()  %>%
#   data.frame()
# 
# DataSumDiff.meand = merge(DataSum2496.meand, MeanMeand0, all.x=T)
# DataSumDiff.meand$DistDiff = DataSumDiff.meand$MnMeander - DataSumDiff.meand$mean0_meand


```


```{r meander single-level Bayesmodels selection, echo=F, eval=F}
# try setting up the linear model in a bayesian form, same as frequentist model above
# use all meander data points, not summarized data
DSmeand_bmoddat = DataSum2496.meand[,c("SDMeander","calcConc","lnCalcConcC","lnCalcConcC2","ExposureHrs","Spp","RepID")]
DSmeand_bmoddat = DSmeand_bmoddat[!is.na(DSmeand_bmoddat$SDMeander),] # drops 26.1% of the data points when using full dataset; none if using summarized dataset 

DSmeand_bmoddat$lnCalcConcC = as.numeric(DSmeand_bmoddat$lnCalcConcC) 
DSmeand_bmoddat$lnCalcConcC2 = as.numeric(DSmeand_bmoddat$lnCalcConcC2) 
DSmeand_bmoddat$SppDummy <- as.numeric(ifelse(DSmeand_bmoddat$Spp=="GS", 0,1))
DSmeand_bmoddat$ExposeDummy <- as.numeric(ifelse(DSmeand_bmoddat$ExposureHrs=="24", 0,1))
#DSmeand_bmoddat$MeanderCircLog = as.numeric(DSmeand_bmoddat$MeanderCircLog) 

# remove fish with only one timepoint of detection, because cant add random slope with one timepoint
DSmeand_bmoddat = DSmeand_bmoddat[!(DSmeand_bmoddat$RepID %in% paste0(c(0,5,100,500,1000,2000), "-WS-4")),]
DSmeand_bmoddat$RepID = as.factor(as.character(DSmeand_bmoddat$RepID))

# let treatment effect and curve vary by species and exposure, and let exposure effect vary by species (all interactions, no multilevel)
set.seed(1983)
meandspp.blm_1 = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bse*(SppDummy)*(ExposeDummy) +
              bt*(lnCalcConcC) +
              bte*(ExposeDummy)*(lnCalcConcC) +
              bts*(SppDummy)*(lnCalcConcC) +
              bt2*(lnCalcConcC2) +
              bt2e*(ExposeDummy)*(lnCalcConcC2) +
              bt2s*(SppDummy)*(lnCalcConcC2) ,
        a ~ dnorm(0,10),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bse ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bte ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2e ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
    sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 1000, warmup = 300, chains = 4)

saveRDS(meandspp.blm_1, "meandspp.blm_1.Rds")


# drop s*e
set.seed(1983)
meandspp.blm_2 = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) +
              bte*(ExposeDummy)*(lnCalcConcC) +
              bts*(SppDummy)*(lnCalcConcC) +
              bt2*(lnCalcConcC2) +
              bt2e*(ExposeDummy)*(lnCalcConcC2) +
              bt2s*(SppDummy)*(lnCalcConcC2) ,
        a ~ dnorm(0,10),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bte ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2e ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
      sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 1000, warmup = 300, chains = 4)

saveRDS(meandspp.blm_2, "meandspp.blm_2.Rds")
## removing s*e improved model by 5.8 AIC points. Will keep it out



# drop all t interactions
set.seed(1983)
meandspp.blm_2b = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) +
              bt2*(lnCalcConcC2),
        a ~ dnorm(0,10),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
      sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 1000, warmup = 300, chains = 4)


# add back in interaction with spp and treatment - below as blm_4

# remove exposure interaction with treatment effect (keep with quadratic)
set.seed(1983)
meandspp.blm_3 = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) +
              bts*(SppDummy)*(lnCalcConcC) +
              bt2*(lnCalcConcC2) +
              bt2e*(ExposeDummy)*(lnCalcConcC2) +
              bt2s*(SppDummy)*(lnCalcConcC2) ,
        a ~ dnorm(0,10),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2e ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
      sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 1000, warmup = 300, chains = 4)

saveRDS(meandspp.blm_3, "meandspp.blm_3.Rds")



# remove exposure interaction with both straight and squadratic treatment effect 
set.seed(1983)
meandspp.blm_4 = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) +
              bts*(SppDummy)*(lnCalcConcC) +
              bt2*(lnCalcConcC2) +
              bt2s*(SppDummy)*(lnCalcConcC2) ,
        a ~ dnorm(0,10),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
      sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 1000, warmup = 300, chains = 4)

saveRDS(meandspp.blm_4, "meandspp.blm_4.Rds")

# remove exposure interaction with both straight and squadratic treatment effect but add bace s*e (match distance model)
set.seed(1983)
meandspp.blm_5 = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bse*SppDummy*ExposeDummy + 
              bt*(lnCalcConcC) +
              bts*(SppDummy)*(lnCalcConcC) +
              bt2*(lnCalcConcC2) +
              bt2s*(SppDummy)*(lnCalcConcC2) ,
        a ~ dnorm(0,10),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bse ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
      sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 1000, warmup = 300, chains = 4)

saveRDS(meandspp.blm_5, "meandspp.blm_5.Rds")
# model without s*e is still better, even when there isn't an interaction between exposure and the treatment and quadratic of treatment. 


# remove species interaction with straight treatment effect 
set.seed(1983)
meandspp.blm_6 = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) +
              bt2*(lnCalcConcC2) +
              bt2s*(SppDummy)*(lnCalcConcC2) ,
        a ~ dnorm(0,10),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
      sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 1000, warmup = 300, chains = 4)

saveRDS(meandspp.blm_6, "meandspp.blm_6.Rds")


compare(meandspp.blm_2,meandspp.blm_3,meandspp.blm_4,meandspp.blm_5,meandspp.blm_6)
```

```{r meander multi-level Baysemodels selection, echo=F, eval=F}

# use all meander data points, not summarized data
DSmeand_bmoddat = DataSum2496.meand[,c("SDMeander","lnCalcConc","lnCalcConc2",
                                       "lnCalcConcC","lnCalcConcC2",
                                     "ExposureHrs","Spp","RepID")]
DSmeand_bmoddat = DSmeand_bmoddat[!is.na(DSmeand_bmoddat$SDMeander),] # drops 26.1% of the data points when using full dataset; none if using summarized dataset 

DSmeand_bmoddat$lnCalcConcC = as.numeric(DSmeand_bmoddat$lnCalcConcC) 
DSmeand_bmoddat$lnCalcConcC2 = as.numeric(DSmeand_bmoddat$lnCalcConcC2) 
DSmeand_bmoddat$SppDummy <- as.numeric(ifelse(DSmeand_bmoddat$Spp=="GS", 0,1))
DSmeand_bmoddat$ExposeDummy <- as.numeric(ifelse(DSmeand_bmoddat$ExposureHrs=="24", 0,1))
#DSmeand_bmoddat$MeanderCircLog = as.numeric(DSmeand_bmoddat$MeanderCircLog) 

# remove fish with only one timepoint of detection, because cant add random slope with one timepoint
DSmeand_bmoddat = DSmeand_bmoddat[!(DSmeand_bmoddat$RepID %in% paste0(c(0,5,100,500,1000,2000), "-WS-4")),]
DSmeand_bmoddat$RepID = as.factor(as.character(DSmeand_bmoddat$RepID))

DSmeand_bmoddat = DSmeand_bmoddat[order(DSmeand_bmoddat$RepID),]

# let treatment effect and curve vary by species and exposure (all interactions except for s*e) and include random intercept for each individual, and let effect of exposure time vary by individual
set.seed(2016)
meandspp.blm_2.rise = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a_fish[RepID] + 
              be_fish[RepID]*ExposeDummy + 
              bs*SppDummy + 
              bse*SppDummy*ExposeDummy + 
              bt*(lnCalcConc) +
              bte*(ExposeDummy)*(lnCalcConc) +
              bts*(SppDummy)*(lnCalcConc) +
              bt2*(lnCalcConc2) +
              bt2e*(ExposeDummy)*(lnCalcConc2) +
              bt2s*(SppDummy)*(lnCalcConc2) ,
           c(a_fish,be_fish)[RepID] ~ dmvnorm2(Mu=c(a,be), sigma=sigma_fish, Rho=Rho), 
                   a ~ dnorm(100,10),
                   be ~ dnorm(0,10),
                   sigma_fish ~ dcauchy(0,10),
                   Rho ~ dlkjcorr(2),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bse ~ dnorm(0,10),
        bte ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2e ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
    sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 2000, warmup = 700, chains = 4)
 
#saveRDS(meandspp.blm_2.rise, "meandspp.blm_2.rise.Rds")

# check model
 precis(meandspp.blm_2.rise, prob = .95, digits=3)
 plot(precis(meandspp.blm_2.rise, prob = .95))
 plot(precis(meandspp.blm_2.rise, prob = .95, depth=2))

 plot(meandspp.blm_2.rise)
 
# the model without removing the tiny meanders (0-1) fit to actual observations TERRIBLY. could be a plotting thing, but I think it's that the estimtd for the curvature of GS are way too high. Same with the basic blm_2 model above, but less so.  

# tried without the tiny meanders, an Rhat/ neff looked better (not great, but better). Sigma has some issues being estimated, but I think the rest look much muhc better than before the filtering.  Hwoever, the model still doesn't fit the data at all. Soemthing weird is going on. 
 
# tried it without centered concentration variables and it worked way better. strange. Still not great. 
 
 
 
set.seed(2019)
meandspp.blm_3.ri = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a_fish[RepID] + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bse*SppDummy*ExposeDummy + 
              bt*(lnCalcConc) +
              bte*(ExposeDummy)*(lnCalcConc) +
              bts*(SppDummy)*(lnCalcConc) +
              bt2*(lnCalcConc2) +
              bt2e*(ExposeDummy)*(lnCalcConc2) +
              bt2s*(SppDummy)*(lnCalcConc2) ,
                  a_fish[RepID] ~ dnorm(a, sigma_fish), 
                     a ~ dnorm(100,10),
                     sigma_fish ~ dcauchy(0,10),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bse ~ dnorm(0,10),
        bte ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2e ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
    sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat, iter = 2000, warmup = 700, chains = 4)
  ## compared this to a model without s*e and this one had an AIC that was 13.3 lower. So keep s*e; I also think it makes the model fit better when overlaid on the points. 

  ## this model (Without random effect for individual slope for exposure time) seems to fit better bawsed on Rhat and neff. The trace plots look cleaner too; not perfect but better. 


 

# use all meander data points, not summarized data
DSmeand_bmoddat_cat = DataSum2496.meand[,c("SDMeander","lnCalcConc","lnCalcConc2",
                                     "ExposureHrs","Spp","RepID")]
DSmeand_bmoddat_cat = DSmeand_bmoddat_cat[!is.na(DSmeand_bmoddat_cat$SDMeander),] # drops 26.1% of the data points when using full dataset; none if using summarized dataset 

DSmeand_bmoddat_cat$GS24dummy <- as.numeric(ifelse(DSmeand_bmoddat_cat$Spp=="GS" & 
                                                 DSmeand_bmoddat_cat$ExposureHrs==24,1,0))
DSmeand_bmoddat_cat$GS96dummy <- as.numeric(ifelse(DSmeand_bmoddat_cat$Spp=="GS" & 
                                                 DSmeand_bmoddat_cat$ExposureHrs==96,1,0))
DSmeand_bmoddat_cat$WS24dummy <- as.numeric(ifelse(DSmeand_bmoddat_cat$Spp=="WS" & 
                                                 DSmeand_bmoddat_cat$ExposureHrs==24,1,0))
DSmeand_bmoddat_cat$WS96dummy <- as.numeric(ifelse(DSmeand_bmoddat_cat$Spp=="WS" & 
                                                 DSmeand_bmoddat_cat$ExposureHrs==96,1,0))

# remove fish with only one timepoint of detection, because cant add random slope with one timepoint
DSmeand_bmoddat_cat = DSmeand_bmoddat_cat[!(DSmeand_bmoddat_cat$RepID %in% paste0(c(0,5,100,500,1000,2000), "-WS-4")),]
DSmeand_bmoddat_cat$RepID = as.factor(as.character(DSmeand_bmoddat_cat$RepID))

DSmeand_bmoddat_cat = DSmeand_bmoddat_cat[order(DSmeand_bmoddat_cat$RepID),] 


set.seed(2019)
meandspp.blm_cat.ri = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a_fish[RepID] + 
      
              bGS24*GS24dummy + 
              bGS96*GS96dummy +
              bWS24*WS24dummy +
              bWS96*WS96dummy +
      
              bt*(lnCalcConc) +
              bt2*(lnCalcConc2) +
      
              bt_GS24*(lnCalcConc)*(GS24dummy) +
              bt2_GS24*(lnCalcConc2)*(GS24dummy) +
              bt_GS96*(lnCalcConc)*(GS96dummy) +
              bt2_GS96*(lnCalcConc2)*(GS96dummy) +
              bt_WS24*(lnCalcConc)*(WS24dummy) +
              bt2_WS24*(lnCalcConc2)*(WS24dummy) +
              bt_WS96*(lnCalcConc)*(WS96dummy) +
              bt2_WS96*(lnCalcConc2)*(WS96dummy),
      
                  a_fish[RepID] ~ dnorm(a, sigma_fish), 
                     a ~ dnorm(100,10),
                     sigma_fish ~ dcauchy(0,10),
        bGS24 ~ dnorm(0,10),
        bGS96 ~ dnorm(0,10),
        bWS24 ~ dnorm(0,10),
        bWS96 ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt_GS24 ~ dnorm(0,10),
        bt_GS96 ~ dnorm(0,10),
        bt_WS24 ~ dnorm(0,10),
        bt_WS96 ~ dnorm(0,10),
        bt2_GS24 ~ dnorm(0,10),
        bt2_GS96 ~ dnorm(0,10),
        bt2_WS24 ~ dnorm(0,10),
        bt2_WS96 ~ dnorm(0,10),
    
    sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat_cat, iter = 2000, warmup = 700, chains = 4)




DSmeand_bmoddat96 = DSmeand_bmoddat[DSmeand_bmoddat$ExposureHrs==96,]


set.seed(2019)
meandspp.blm_96 = map2stan(
  alist(
    SDMeander ~ dnorm(mu, sigma), 
      mu  <-  a +
              #a_tray[RepID] +
              bs*SppDummy + 
              bt*(lnCalcConc) +
              bts*(SppDummy)*(lnCalcConc) +
              bt2*(lnCalcConc2) +
              bt2s*(SppDummy)*(lnCalcConc2) ,
        a ~ dnorm(100,10),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2s ~ dnorm(0,10),
    sigma ~ dcauchy(0,10)
    ) ,
  data = DSmeand_bmoddat96, iter = 2000, warmup = 700, chains = 4)
 
```
### _2.rise is a logical model, and pretty well matches the other models, and lets us test the questions we're interested in. But the neff for sigma is pretty low. Tried various levels of dcauchy(0,X) and estimates didn't change too dramatically: with dcauchy(0,20) neff=26. With dcauchy(0,50) neff=4. with dcauchy(0,5) neff=30. with dcauchy(0,10) neff=25. Rhat ~1.1 - 1.2 for most also. Traceplots look okay but not perfect. Some sections where one or two of the chains don't mix (ie: are pretty thin lines). And sigma trace plots don't look great. But likely an okay model to use anyway. 

### tried with a prior for alpha that was (100,10) (mean SDMeader was 195). And that alone didn't help much

### tried filtering out the tiny meanders, because there were a lot and it seems like they could be within the range of error of the computer tracking.  

### Unfortunately, none of these changes to make the model converge better have improved the fit with the raw data. The curve of the GS preduction is too big, and potentially even in the wrong direction (should be concave but it's convex). And the WS is too flat when it should be curved. It's almost as though the bt2 element was added to the wrong place....hm. 
 
### tried it without centering the variables for concentration; I coulnd't wrap my head around how a quadratic would work with centered vars. Tried a 'guess and check' method to see if I could pick parameters that would make approximately the right shape, and I couldn't find them 
 
### Comparing the models with and wihtout random effects, we see that when random effects are added the estimates all move closer to zero (shrinkage?) and the variance gets arger for alle stimates that do not include any effect of exposure time (since that is the random slope). Those which do include exposure time show reduced sd values because some of the variation assocaited with exposure time is accounted for by the individual clustering. I think this all makes sense


```{r meander multi-level Bayesmodels predict, echo=F, eval=F}

# pull posterior from model:
set.seed(42)
post = extract.samples(meandspp.blm_96)

# set up dataframe to plot posterior predictions
# preddat = list(CalcConc=rep(seq(0, 2250, 2), 4), 
#                lnCalcConc=log(rep(seq(0, 2250, 2),4)+1), 
#                lnCalcConc2=log(rep(seq(0, 2250, 2),4)+1) ^2, 
#                lnCalcConcC=   (log(rep(seq(0, 2250, 2),4)+1)) - 
#                  mean(DataSum2496.dist$lnCalcConc) ,
#                lnCalcConcC2=( (log(rep(seq(0, 2250, 2),4)+1)) - 
#                  mean(DataSum2496.dist$lnCalcConc) ) ^ 2 ,               
#                ExposeDummy = rep(c(0,1), each=1126*2), 
#                SppDummy = rep(rep(c(0,1),each=1126),2), 
#                RepID = rep(16,4504))

preddat = list(CalcConc=rep(seq(0, 2250, 2), 2), 
               lnCalcConc=log(rep(seq(0, 2250, 2),2)+1), 
               lnCalcConc2=log(rep(seq(0, 2250, 2),2)+1) ^2, 
               lnCalcConcC=   (log(rep(seq(0, 2250, 2),2)+1)) - 
                 mean(DataSum2496.dist$lnCalcConc) ,
               lnCalcConcC2=( (log(rep(seq(0, 2250, 2),2)+1)) - 
                 mean(DataSum2496.dist$lnCalcConc) ) ^ 2 ,               
               SppDummy = rep(c(0,1), each=1126))

# replace individual fish estimates with global estimate; this should predict to the average individual in a population...so since I want to get at the population level response this seems most appropriate.
a_global = matrix(post$a,5200,90) # 5200 = 1300 samples per chain x 4 chains
be_global = matrix(post$be,5200,90)

# use link to predict to mean only
link.2.rise <- link(meandspp.blm_96, n=1000, data= preddat)#, 
                        replace = list(a_fish = a_global)), 
                                        be_fish = be_global) )

preddat.df <- data.frame(sapply(preddat,c)) # thanks stack overflow! https://stackoverflow.com/questions/4227223/convert-a-list-to-a-data-frame

preddat.df$link_mu_mn <- apply(link.2.rise, 2, mean) 
preddat.df$link_PI05 = apply(link.2.rise, 2, PI, .95)[1,] 
preddat.df$link_PI95 = apply(link.2.rise, 2, PI, .95)[2,] 


  # plot back-transformed data
  preddat.df$Spp <- ifelse(preddat.df$SppDummy==0,"GS","WS")
  preddat.df$ExposureHrs <- ifelse(preddat.df$ExposeDummy==0,"24","96")


MeandPred_transf_plot = ggplot() + 
  geom_point(data=DSmeand_bmoddat96,
             aes(x=(lnCalcConc),                          
                 y=SDMeander, 
                 color=factor(Spp))) + 
  geom_ribbon(data = preddat.df, aes(x=(lnCalcConc), 
              ymin=link_PI05, ymax=link_PI95), 
              alpha=.4, col="grey70")+
  geom_line(data = preddat.df, aes(x=(lnCalcConc), y=link_mu_mn), lwd=.2)+
    facet_grid(Spp~ExposureHrs, scales="free", labeller = labeller(
      ExposureHrs = c("24"="24 Hours Exposure","96"="96 Hours Exposure"),
      Spp = c("GS" = "Green Sturgeon", "WS" = "White Sturgeon")))+
    scale_color_manual(values=c("green3", "steelblue3"), guide=FALSE)+
    ylab("SD of Track Meander (deg/mm)") + 
    scale_x_continuous(name="Log-centered Bifenthrin Concentration (ng/L)") + #, breaks = seq(-7.5,5, 2.5), labels=(seq(-7.5,5, 2.5)+3.73) ) +
    theme_bw()

MeandPred_transf_plot96 = ggplot() + 
  geom_point(data=DataSum2496.meand,
             aes(x=(lnCalcConcC),                          
                 y=SDMeander, 
                 color=factor(Spp))) + 
  geom_ribbon(data = preddat.df, aes(x=(lnCalcConcC), 
              ymin=link_PI05, ymax=link_PI95), 
              alpha=.4, col="grey70")+
  geom_line(data = preddat.df, aes(x=(lnCalcConcC), y=link_mu_mn), lwd=.2)+
    facet_wrap(Spp~., scales="free", labeller = labeller(
      Spp = c("GS" = "Green Sturgeon", "WS" = "White Sturgeon")))+
    scale_color_manual(values=c("green3", "steelblue3"), guide=FALSE)+
    ylab("SD of Track Meander (deg/mm)") + 
    scale_x_continuous(name="Log-centered Bifenthrin Concentration (ng/L)") + #, breaks = seq(-7.5,5, 2.5), labels=(seq(-7.5,5, 2.5)+3.73) ) +
    theme_bw()


MeandPred_transf_plot_smooth = ggplot(data=DataSum2496.meand,
             aes(x=(lnCalcConcC),                          
                 y=SDMeander, 
                 color=factor(Spp))) + geom_point() + geom_smooth(span=1.25) + 
    facet_grid(Spp~ExposureHrs, scales="free", labeller = labeller(
      ExposureHrs = c("24"="24 Hours Exposure","96"="96 Hours Exposure"),
      Spp = c("GS" = "Green Sturgeon", "WS" = "White Sturgeon")))+
    scale_color_manual(values=c("green3", "steelblue3"), guide=FALSE)+
    ylab("SD of Track Meander (deg/mm)") + 
    scale_x_continuous(name="Log-centered Bifenthrin Concentration (ng/L)") + #, breaks = seq(-7.5,5, 2.5), labels=(seq(-7.5,5, 2.5)+3.73) ) +
    theme_bw()

MeandPred_orig_plot = ggplot() + 
  geom_point(data=DataSum2496.meand,
             aes(x=exp(lnCalcConc),                          
                 y=SDMeander, 
                 color=factor(Spp))) + 
  geom_ribbon(data = preddat.df, aes(x=(CalcConc), 
              ymin=link_PI05, ymax=link_PI95), 
              alpha=.4, col="grey70")+
  geom_line(data = preddat.df, aes(x=(CalcConc), y=link_mu_mn), lwd=.2)+
  facet_grid(Spp~ExposureHrs, scales="free", labeller = labeller(
    ExposureHrs = c("24"="24 Hours Exposure","96"="96 Hours Exposure"),
    Spp = c("GS" = "Green Sturgeon", "WS" = "White Sturgeon")))+
  scale_color_manual(values=c("green3", "steelblue3"), guide=FALSE)+
  ylab("SD of Track Meander (deg/mm)") + 
  scale_x_continuous(name="Bifenthrin Concentration (ng/L)") + #, breaks = seq(-7.5,5, 2.5), labels=(seq(-7.5,5, 2.5)+3.73) ) +
  theme_bw()

MeandPred_transf_plot_smooth
MeandPred_transf_plot
MeandPred_orig_plot 

```
 
 
### MeanderSD Contrasts

### Study questions: 

###  1) is there an effect (all treatment vs control), and which metrics show an effect

###  2) does that effect change with time [96 treat vs 96 control and 24 treat vs 24 control OR individual change from 24 to 96 (all treatments) as compared to individual change for control between 24 and 96]

###  3) Does the magnitude of the effect vary between species

```{r meander multi-level model density plots, echo=F}
set.seed(1983) 
postmeand = extract.samples(meandspp.blm_2.rise)  

# reference table to convert from treatment to centered, log-transformed measured concentrations
TreatCalcTable = unique(DataSum2496.meand[order(DataSum2496.meand$Treatment)
                                             ,c("Spp", "Treatment","lnCalcConc", "lnCalcConcC")])
TreatCalcTable$lnCalcConcC2 = TreatCalcTable$lnCalcConcC ^ 2



# calculate differences between control and treatment at each time point for each species (20 contrasts)
GS0bif24m = postmeand$a +
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==0)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==0)$lnCalcConcC2)
GS5bif24m = postmeand$a + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==5)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==5)$lnCalcConcC2)
GS100bif24m = postmeand$a + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==100)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==100)$lnCalcConcC2)
GS500bif24m = postmeand$a + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==500)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==500)$lnCalcConcC2)
GS1000bif24m = postmeand$a +
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==1000)$lnCalcConcC) + 
  postmeand$bt2* as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==1000)$lnCalcConcC2)
GS2000bif24m = postmeand$a + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==2000)$lnCalcConcC) + 
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==2000)$lnCalcConcC2)

mean(GS0bif24m)
mean(GS5bif24m)
 cont.GS_0.5bif_24m = (sum(GS0bif24m<GS5bif24m) / length(GS0bif24m)) 
 cont.GS_0.5bif_24m #72.0% prob that 5 is more than 0
mean(GS100bif24m)
 cont.GS_0.100bif_24m = (sum(GS0bif24m<GS100bif24m) / length(GS0bif24m)) 
 cont.GS_0.100bif_24m #62.4% prob that 100 is more than 0
mean(GS500bif24m)
 cont.GS_0.500bif_24m = (sum(GS0bif24m<GS500bif24m) / length(GS0bif24m)) 
 cont.GS_0.500bif_24m #40.7% prob that 500 is more than 0
mean(GS1000bif24m)
 cont.GS_0.1000bif_24m = (sum(GS0bif24m<GS1000bif24m) / length(GS0bif24m)) 
 cont.GS_0.1000bif_24m #32.8% prob that 1000 is more than 0
mean(GS2000bif24m)
 cont.GS_0.2000bif_24m = (sum(GS0bif24m<GS2000bif24m) / length(GS0bif24m)) 
 cont.GS_0.2000bif_24m #13.7% prob that 2000 is more than 0 (86.3% chance that 2000 is less than 0)

GS24m.plot = ggplot() + geom_density(aes(x=GS0bif24m, color="black")) + 
  geom_density(aes(x=GS5bif24m, color="green3")) + 
  geom_density(aes(x=GS100bif24m, color="goldenrod")) + 
  geom_density(aes(x=GS500bif24m, color="red3")) + 
  geom_density(aes(x=GS1000bif24m, color="pink")) + 
  geom_density(aes(x=GS2000bif24m, color="steelblue3")) + 
  scale_color_manual(name="Nominal Bifenthrin\nConcentration", 
                     values=c("black"="black", "green3"="green3", "goldenrod"="goldenrod",
                              "red3"="red3", "pink"="pink","steelblue3"="steelblue3"), 
                     labels=c("0 ng/L", "5 ng/L", "100ng/L", 
                              "500ng/L", "1000ng/L", "2000 ng/L") ) + 
  xlab("SD Track Meander (GS @ 24hr)")+
  theme_bw()


# repeat for green sturgeon, 96hr

GS0bif96m = postmeand$a + postmeand$be +
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==0)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==0)$lnCalcConcC2)
GS5bif96m = postmeand$a + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==5)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==5)$lnCalcConcC2)
GS100bif96m = postmeand$a + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==100)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==100)$lnCalcConcC2)
GS500bif96m = postmeand$a + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==500)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==500)$lnCalcConcC2)
GS1000bif96m = postmeand$a + postmeand$be +
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==1000)$lnCalcConcC) + 
  postmeand$bt2* as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==1000)$lnCalcConcC2)
GS2000bif96m = postmeand$a + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==2000)$lnCalcConcC) + 
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==2000)$lnCalcConcC2)

mean(GS0bif96m)
mean(GS5bif96m)
 cont.GS_0.5bif_96m = (sum(GS0bif96m<GS5bif96m) / length(GS0bif96m)) 
 cont.GS_0.5bif_96m #72.0% prob that 5 is more than 0
mean(GS100bif96m)
mean(GS500bif96m)
mean(GS1000bif96m)
mean(GS2000bif96m)

GS96m.plot = ggplot() + geom_density(aes(x=GS0bif96m), color="black") + 
  geom_density(aes(x=GS5bif96m), color="green3") + 
  geom_density(aes(x=GS100bif96m), color="goldenrod") + 
  geom_density(aes(x=GS500bif96m), color="red3") + 
  geom_density(aes(x=GS1000bif96m), color="pink") + 
  geom_density(aes(x=GS2000bif96m), color="steelblue3") + 
  scale_color_manual(name="Nominal Bifenthrin\nConcentration", 
                     values=c("black"="black", "green3"="green3", "goldenrod"="goldenrod",
                              "red3"="red3", "pink"="pink","steelblue3"="steelblue3"), 
                     labels=c("0 ng/L", "5 ng/L", "100ng/L", 
                              "500ng/L", "1000ng/L", "2000 ng/L") ) + 
  xlab("SD Track Meander (GS @ 96hr)")+
  theme_bw()


# repeat for white sturgeon, 24hr

WS0bif24m = postmeand$a + postmeand$bs +
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC2) +
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC) +
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC2)
WS5bif24m = postmeand$a + postmeand$bs + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC2)+
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC) +
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC2)
WS100bif24m = postmeand$a + postmeand$bs + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC2)+
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC) +
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC2)
WS500bif24m = postmeand$a + postmeand$bs + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC2)+
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC) +
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC2)
WS1000bif24m = postmeand$a + postmeand$bs +
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC) + 
  postmeand$bt2* as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC2)+
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC) + 
  postmeand$bts2* as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC2)
WS2000bif24m = postmeand$a + postmeand$bs + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC) + 
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC2)+
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC) + 
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC2)

mean(WS0bif24m)
mean(WS5bif24m)
 cont.WS_0.5bif_24m = (sum(WS0bif24m<WS5bif24m) / length(WS0bif24m)) 
 cont.WS_0.5bif_24m #100% prob that 5 is more than 0
mean(WS100bif24m)
 cont.WS_0.100bif_24m = (sum(WS0bif24m<WS100bif24m) / length(WS0bif24m)) 
 cont.WS_0.100bif_24m #100% prob that 100 is more than 0
mean(WS500bif24m)
 cont.WS_0.500bif_24m = (sum(WS0bif24m<WS500bif24m) / length(WS0bif24m)) 
 cont.WS_0.500bif_24m #55% prob that 500 is more than 0
mean(WS1000bif24m)
 cont.WS_0.1000bif_24m = (sum(WS0bif24m<WS1000bif24m) / length(WS0bif24m)) 
 cont.WS_0.1000bif_24m #0% prob that 1000 is more than 0 (i.e. 100% prob that 1000 is less than 0)
mean(WS2000bif24m)
 cont.WS_0.2000bif_24m = (sum(WS0bif24m<WS2000bif24m) / length(WS0bif24m)) 
 cont.WS_0.2000bif_24m #0% prob that 2000 is more than 0 (i.e. 100% prob that 1000 is less than 0)
 
WS24m.plot = ggplot() + geom_density(aes(x=WS0bif24m), color="black") + 
  geom_density(aes(x=WS5bif24m), color="green3") + 
  geom_density(aes(x=WS100bif24m), color="goldenrod") + 
  geom_density(aes(x=WS500bif24m), color="red3") + 
  geom_density(aes(x=WS1000bif24m), color="pink") + 
  geom_density(aes(x=WS2000bif24m), color="steelblue3") + 
  scale_color_manual(name="Nominal Bifenthrin\nConcentration", 
                     values=c("black"="black", "green3"="green3", "goldenrod"="goldenrod",
                              "red3"="red3", "pink"="pink","steelblue3"="steelblue3"), 
                     labels=c("0 ng/L", "5 ng/L", "100ng/L", 
                              "500ng/L", "1000ng/L", "2000 ng/L") ) + 
  xlab("SD Track Meander (WS @ 24hr)")+
  theme_bw()


# repeat for white sturgeon, 96hr


WS0bif96m = postmeand$a + postmeand$bs + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC2) +
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC) +
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC2)
WS5bif96m = postmeand$a + postmeand$bs + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC2) +
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC) +
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC2)
WS100bif96m = postmeand$a + postmeand$bs + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC2) +
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC) +
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC2)
WS500bif96m = postmeand$a + postmeand$bs + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC) +
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC2) +
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC) +
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC2)
WS1000bif96m = postmeand$a + postmeand$bs + postmeand$be +  
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC) + 
  postmeand$bt2* as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC2) +
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC) + 
  postmeand$bts2* as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC2)
WS2000bif96m = postmeand$a + postmeand$bs + postmeand$be + 
  postmeand$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC) + 
  postmeand$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC2) +
  postmeand$bts1*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC) + 
  postmeand$bts2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC2)

mean(WS0bif96m)
mean(WS5bif96m)
 cont.WS_0.5bif_96m = (sum(WS0bif96m<WS5bif96m) / length(WS0bif96m)) 
 cont.WS_0.5bif_96m #100% prob that 5 is more than 0
mean(WS100bif96m)
 cont.WS_0.100bif_96m = (sum(WS0bif96m<WS100bif96m) / length(WS0bif96m)) 
 cont.WS_0.100bif_96m #100% prob that 100 is more than 0
mean(WS500bif96m)
 cont.WS_0.500bif_96m = (sum(WS0bif96m<WS500bif96m) / length(WS0bif96m)) 
 cont.WS_0.500bif_96m #55.0% prob that 500 is more than 0
mean(WS1000bif96m)
 cont.WS_0.1000bif_96m = (sum(WS0bif96m<WS1000bif96m) / length(WS0bif96m)) 
 cont.WS_0.1000bif_96m #0% prob that 1000 is more than 0 (i.e. 100% prob that 1000 is less than 0)
mean(WS2000bif96m)
 cont.WS_0.2000bif_96m = (sum(WS0bif96m<WS2000bif96m) / length(WS0bif96m)) 
 cont.WS_0.2000bif_96m #0% prob that 2000 is more than 0 (i.e. 100% prob that 2000 is less than 0)

WS96m.plot = ggplot() + geom_density(aes(x=WS0bif96m), color="black") + 
  geom_density(aes(x=WS5bif96m), color="green3") + 
  geom_density(aes(x=WS100bif96m), color="goldenrod") + 
  geom_density(aes(x=WS500bif96m), color="red3") + 
  geom_density(aes(x=WS1000bif96m), color="pink") + 
  geom_density(aes(x=WS2000bif96m), color="steelblue3") + 
  scale_color_manual(name="Nominal Bifenthrin\nConcentration", 
                     values=c("black"="black", "green3"="green3", "goldenrod"="goldenrod",
                              "red3"="red3", "pink"="pink","steelblue3"="steelblue3"), 
                     labels=c("0 ng/L", "5 ng/L", "100ng/L", 
                              "500ng/L", "1000ng/L", "2000 ng/L") ) + 
  xlab("SD Track Meander (WS @ 96hr)")+
  theme_bw()

GS24m.plot + GS96m.plot + WS24m.plot + WS96m.plot

predgroups = list(GS0bif24m, GS5bif24m,GS100bif24m,GS500bif24m,GS1000bif24m,GS2000bif24m,
                  GS0bif96m, GS5bif96m,GS100bif96m,GS500bif96m,GS1000bif96m,GS2000bif96m,
                  WS0bif24m, WS5bif24m,WS100bif24m,WS500bif24m,WS1000bif24m,WS2000bif24m,
                  WS0bif96m, WS5bif96m,WS100bif96m,WS500bif96m,WS1000bif96m,WS2000bif96m)
PredGroupNames =  as.character(expression(GS0bif24m,GS5bif24m,GS100bif24m,GS500bif24m,
                                          GS1000bif24m,GS2000bif24m,
                   GS0bif96m, GS5bif96m,GS100bif96m,GS500bif96m,GS1000bif96m,GS2000bif96m,
                   WS0bif24m, WS5bif24m,WS100bif24m,WS500bif24m,WS1000bif24m,WS2000bif24m,
                   WS0bif96m, WS5bif96m,WS100bif96m,WS500bif96m,WS1000bif96m,WS2000bif96m))
posterior.preds = data.frame(PredGroupNames = PredGroupNames, 
                             PredMeans = sapply(predgroups, mean),
                             Pred95CIl = sapply(predgroups, quantile, 0.05),
                             Pred95CIu = sapply(predgroups, quantile, 0.95))
```
#### if use this in a paper, recolor to a more intuitive gradient and add legend
 
 
```{r meander multi-level model time and spp contrasts, echo=F}
# this uses the predicted posteriors for distance moved, calculated above

# GS vs WS
cont.GS.WS_0bif_24m = sum(GS0bif24m<WS0bif24m) / length(GS0bif24m)
 cont.GS.WS_0bif_24m # 100% probability that GS will move less than WS

 cont.GS.WS_0bif_96m = sum(GS0bif96m<WS0bif96m) / length(GS0bif96m)
 cont.GS.WS_0bif_96m # 100% probability that GS will move less than WS
cont.GS.WS_2000bif_96m = sum(GS2000bif96m<WS2000bif96m) / length(GS2000bif96m)
 cont.GS.WS_2000bif_96m # 99.89% probability that GS will move less than WS


# Green Sturgeon, Exposure Hr
cont.GS0bif_96.24m = (sum(GS0bif96m<GS0bif24m) / length(GS0bif96m)) 
cont.GS5bif_96.24m = (sum(GS5bif96m<GS5bif24m) / length(GS5bif96m)) 
cont.GS100bif_96.24m = (sum(GS100bif96m<GS100bif24m) / length(GS100bif96m)) 
cont.GS500bif_96.24m = (sum(GS500bif96m<GS500bif24m) / length(GS500bif96m)) 
cont.GS1000bif_96.24m = (sum(GS1000bif96m<GS1000bif24m) / length(GS1000bif96m)) 
cont.GS2000bif_96.24m = (sum(GS2000bif96m<GS2000bif24m) / length(GS2000bif96m)) 

cont.GS0bif_96.24m
cont.GS5bif_96.24m
cont.GS100bif_96.24m
cont.GS500bif_96.24m
cont.GS1000bif_96.24m
cont.GS2000bif_96.24m


GS0bif_96.24m.plot = ggplot() + geom_density(aes(x=GS0bif96m), color="steelblue3") + 
  geom_density(aes(x=GS0bif24m), color="green3") + 
  xlab("SD Meander - GS @ 0 ng/L bifenthrin\n24 vs 96 hrs)") +
  theme_bw()
GS2000bif_96.24m.plot = ggplot() + geom_density(aes(x=GS2000bif96m), color="steelblue3") + 
  geom_density(aes(x=GS2000bif24m), color="green3") + 
  xlab("SD Meander - GS @ 2000 ng/L bifenthrin\n24 vs 96 hrs)") +
  theme_bw()
# can repeat these for each concentration if interesting

# use patchwork grammar to plot multiple plots together
GS0bif_96.24m.plot + GS2000bif_96.24m.plot   




# White Sturgeon: probability of a difference between 24 and 96 hours, at each treatment level
cont.WS0bif_96.24m = (sum(WS0bif96m<WS0bif24m) / length(WS0bif96m)) 
cont.WS0bif_96.24m


WS0bif_96.24m.plot = ggplot() + geom_density(aes(x=WS0bif96m), color="steelblue3") + 
  geom_density(aes(x=WS0bif24m), color="green3") + 
  xlab("SD Meander - WS @ 0 ng/L bifenthrin\n24 vs 96 hrs)") +
  theme_bw()
WS0bif_96.24m.plot


```

#### well, these plots all look about the same, as they should since there's no interaction for concentration * exposure time; which fits the contrast value - it is 95.2% likely (for all comparisons of concentation, because there isn't an interaction) that for Green Sturgeon the distance moved at 96hrs (7ph) is greater than the distance moved at 24hrs (4 dph). But for white sturgeon, it's a 100% chance that there is less movement at 96hrs than at 24hrs exposure. 




########### 
     
## Center Zone Models - good and complete! #####
```{r center zone data, echo=F, eval=F} 
DataSum2496.centC = DataSum2496.raw %>%
  filter(!is.na(InZoneC)) %>%  ### will this bias the data if I remove those points where there isn't a detection? If the fish is more likely to be in the center when it's stationary, and it's more likely to be undetected when it's stationary, this may bias these values and under-represent the effect of the bifenthrin (more stationary when strongly affected)
  # also note, this line doesn't actually change the results because summing the InZoneC column drops all NA automatically; this line above just makes that explicit
  group_by(index, Trial, Arena, Replicate, ExposureHrs, Treatment, calcConc, Spp, RepID) %>%
  summarize(InZoneC = sum(InZoneC, na.rm=T), InZoneB = sum(InZoneB, na.rm=T), npos = n()) %>%
  mutate(PercZoneC1 = InZoneC / npos) %>%
  mutate(PercZoneC2 = InZoneC / (InZoneC+InZoneB)) %>%
  # ran this comparison to see how much the NA can influence the data, and it seems to be minor
  ungroup()  %>%
  data.frame()

 DataSum2496.centC$lnCalcConc = log(DataSum2496.centC$calcConc+1)
 DataSum2496.centC$lnCalcConcC= scale(DataSum2496.centC$lnCalcConc, scale=FALSE)
 DataSum2496.centC$lnCalcConcC2 = DataSum2496.centC$lnCalcConcC^2 
    # order of transformations matters (log then scale then square)

 # add a tiny amount to the perc to keep it above zero (issues for fitting because logit(0)=Inf
 # but don't add so much that the max will be =>1
 DataSum2496.centC$PercZoneC1 = DataSum2496.centC$PercZoneC1 +.001
 DataSum2496.centC$PercZoneC2 = DataSum2496.centC$PercZoneC2 +.001
 
```

```{r center zone data 2, echo=F} 
DataSum2496.centC = DataSum2496.raw %>%
  filter(!is.na(InZoneC)) %>%  ### will this bias the data if I remove those points where there isn't a detection? If the fish is more likely to be in the center when it's stationary, and it's more likely to be undetected when it's stationary, this may bias these values and under-represent the effect of the bifenthrin (more stationary when strongly affected)
  # also note, this line doesn't actually change the results because summing the InZoneC column drops all NA automatically; this line above just makes that explicit
  group_by(index, Trial, Arena, Replicate, ExposureHrs, Treatment, calcConc, Spp, RepID) %>%
  summarize(nCent = sum(InZoneC), npos = n()) %>%
  ungroup()  %>%
  data.frame()

 DataSum2496.centC$lnCalcConc = log(DataSum2496.centC$calcConc+1)
 DataSum2496.centC$lnCalcConcC= scale(DataSum2496.centC$lnCalcConc, scale=FALSE)
 DataSum2496.centC$lnCalcConcC2 = DataSum2496.centC$lnCalcConcC^2 
    # order of transformations matters (log then scale then square)

 DataSum2496.centC$nOuter = DataSum2496.centC$npos - DataSum2496.centC$nCent
 
 
```


```{r centerzone multi-level Bayesmodels selection, echo=F, eval=F}
# try setting up the linear model in a bayesian form, same as frequentist model above
# use all meander data points, not summarized data
DScent_bmoddat = DataSum2496.centC[,c("nCent", "npos", "lnCalcConcC","lnCalcConcC2",
                                     "ExposureHrs","Spp","RepID")]
DScent_bmoddat$lnCalcConcC = as.numeric(DScent_bmoddat$lnCalcConcC) 
DScent_bmoddat$lnCalcConcC2 = as.numeric(DScent_bmoddat$lnCalcConcC2) 
DScent_bmoddat$SppDummy <- as.numeric(ifelse(DScent_bmoddat$Spp=="GS", 0,1))
DScent_bmoddat$ExposeDummy <- as.numeric(ifelse(DScent_bmoddat$ExposureHrs=="24", 0,1))

# remove fish with only one timepoint of detection, because cant add random slope with one timepoint
DScent_bmoddat = DScent_bmoddat[!(DScent_bmoddat$RepID %in% paste0(c(0,5,100,500,1000,2000), "-WS-4")),]
DScent_bmoddat$RepID = as.factor(as.character(DScent_bmoddat$RepID))


set.seed(1983)
centspp.blm_int_a = map2stan(
  alist(
    nCent ~ dbinom(npos, p), 
      logit(p)  <- a_fish[RepID] + 
              be*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) ,
    
           a_fish[RepID] ~ dnorm(a,sigma_fish),
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           sigma_fish ~ dcauchy(0,2),
        be ~ dnorm(0,10),
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)
# wanders around the parameter space a lot. Doens't look very good at all. Hm. 

set.seed(1983)
centspp.blm_int1 = map2stan(
  alist(
    nCent ~ dbinom(npos, p), 
      logit(p)  <- a_fish[RepID] + 
              be_fish[RepID]*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) ,
    
           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),# 
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope
    
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)

set.seed(1983)
centspp.blm_int2 = map2stan(
  alist(
    nCent ~ dbinom(npos, p), 
      logit(p)  <-  a_fish[RepID] + 
              be_fish[RepID]*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) + 
              bt2*(lnCalcConcC2), 
      
           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),# 
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope
    
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bt2 ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)

set.seed(1983)
centspp.blm_int3 = map2stan(
  alist(
      nCent ~ dbinom(npos, p), 
      logit(p)  <-  a_fish[RepID] + 
              be_fish[RepID]*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) + 
              bt2*(lnCalcConcC2) + 
              bt2e*(lnCalcConcC2)*(ExposeDummy), 
      
           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),# 
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope
    
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2e ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)

set.seed(1983)
centspp.blm_int4 = map2stan(
  alist(
    nCent ~ dbinom(npos, p), 
      logit(p)  <- a_fish[RepID] + 
              be_fish[RepID]*ExposeDummy + 
              bs*SppDummy + 
              bt*(lnCalcConcC) + 
              bt2*(lnCalcConcC2) + 
              bt2s*(lnCalcConcC2)*(SppDummy), 
       
           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),# 
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope
    
        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2s ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)
## 2294 transitions after warmup that exceeded max treedepth; wants more sampling too

set.seed(1983)
centspp.blm_int5 = map2stan(
  alist(
    nCent ~ dbinom(npos, p),
      logit(p)  <- a_fish[RepID] +
              be_fish[RepID]*ExposeDummy +
              bs*SppDummy +
              bt*(lnCalcConcC) +
              bte*(lnCalcConcC)*(ExposeDummy)+
              bt2*(lnCalcConcC2) +
              bt2s*(lnCalcConcC2)*(SppDummy),

           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),#
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope

        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bte ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2s ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)


set.seed(1983)
centspp.blm_int6 = map2stan(
  alist(
    nCent ~ dbinom(npos, p),
      logit(p)  <- a_fish[RepID] +
              be_fish[RepID]*ExposeDummy +
              bs*SppDummy +
              bt*(lnCalcConcC) +
              bte*(lnCalcConcC)*(ExposeDummy)+
              bts*(lnCalcConcC)*(SppDummy),

           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),#
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope

        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bte ~ dnorm(0,10),
        bts ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)

set.seed(1983)
centspp.blm_int7 = map2stan(
  alist(
    nCent ~ dbinom(npos, p),
      logit(p)  <- a_fish[RepID] +
              be_fish[RepID]*ExposeDummy +
              bs*SppDummy +
              bt*(lnCalcConcC) +
              bse*(SppDummy)*(ExposeDummy)+
              bte*(lnCalcConcC)*(ExposeDummy)+
              bts*(lnCalcConcC)*(SppDummy)+
              bt2*(lnCalcConcC2) +
              bt2e*(lnCalcConcC2)*(ExposeDummy)+
              bt2s*(lnCalcConcC2)*(SppDummy),

           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),#
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope

        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bse ~ dnorm(0,10),
        bte ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2e ~ dnorm(0,10),
        bt2s ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)

set.seed(1983)
centspp.blm_int8 = map2stan(
  alist(
    nCent ~ dbinom(npos, p),
      logit(p)  <- a_fish[RepID] +
              be_fish[RepID]*ExposeDummy +
              bs*SppDummy +
              bt*(lnCalcConcC) +
              bse*(SppDummy)*(ExposeDummy)+
              bts*(lnCalcConcC)*(SppDummy)+
              bt2*(lnCalcConcC2) +
              bt2s*(lnCalcConcC2)*(SppDummy),

           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),#
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope

        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bse ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2s ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)

compare(centspp.blm_int1, centspp.blm_int2, centspp.blm_int3, centspp.blm_int4, centspp.blm_int5, centspp.blm_int6, centspp.blm_int7,centspp.blm_int8)
## models are basically all the same...8 is slightly better WAIC than 7 (1.6) suggesting there isn't a need for the interacgtion between treatment and exposure period. But 7 is the only one that is >2 different from the 'best' which are models 2 and 4

```

```{r centerzone multi-level Bayesmodels final, echo=F}
DScent_bmoddat = DataSum2496.centC[,c("nCent", "npos", "lnCalcConcC","lnCalcConcC2", "ExposureHrs","Spp","RepID")]
DScent_bmoddat$lnCalcConcC = as.numeric(DScent_bmoddat$lnCalcConcC) 
DScent_bmoddat$lnCalcConcC2 = as.numeric(DScent_bmoddat$lnCalcConcC2) 
DScent_bmoddat$SppDummy <- as.numeric(ifelse(DScent_bmoddat$Spp=="GS", 0,1))
DScent_bmoddat$ExposeDummy <- as.numeric(ifelse(DScent_bmoddat$ExposureHrs=="24", 0,1))

# remove fish with only one timepoint of detection, because cant add random slope with one timepoint
DScent_bmoddat = DScent_bmoddat[!(DScent_bmoddat$RepID %in% paste0(c(0,5,100,500,1000,2000), "-WS-4")),]
DScent_bmoddat$RepID = as.factor(as.character(DScent_bmoddat$RepID))



set.seed(1983)
centspp.blm_int8 = map2stan(
  alist(
    nCent ~ dbinom(npos, p),
      logit(p)  <- a_fish[RepID] +
              be_fish[RepID]*ExposeDummy +
              bs*SppDummy +
              bt*(lnCalcConcC) +
              bse*(SppDummy)*(ExposeDummy)+
              bts*(lnCalcConcC)*(SppDummy)+
              bt2*(lnCalcConcC2) +
              bt2s*(lnCalcConcC2)*(SppDummy),

           c(a_fish,be_fish)[RepID] ~ dmvnorm2(c(a, be),sigma_fish, Rho),#
           a ~ dnorm(4,10), # approximate mean of base case (GS-24hr)
           be ~ dnorm(0,10),
           sigma_fish ~ dcauchy(0,2),
           Rho ~ dlkjcorr(2), # not as flat as (1) but biases against strong correlations of intercept and slope

        bs ~ dnorm(0,10),
        bt ~ dnorm(0,10),
        bse ~ dnorm(0,10),
        bts ~ dnorm(0,10),
        bt2 ~ dnorm(0,10),
        bt2s ~ dnorm(0,10)
    ) ,
  data = DScent_bmoddat, iter = 2000, warmup = 300, chains = 3)


# check model
 precis(centspp.blm_int8, prob = .95, digits=3)
 plot(precis(centspp.blm_int8, prob = .95))
 plot(precis(centspp.blm_int8, prob = .95, depth=2))

 plot(centspp.blm_int8)
```

```{r center zone multi-level Bayesmodels predict, echo=F, eval=F}

# pull posterior from model:
set.seed(42)
post = extract.samples(centspp.blm_int8)

# set up dataframe to plot posterior predictions
preddat = list(CalcConc=rep(seq(0, 2250, 2), 4), 
               lnCalcConc=log(rep(seq(0, 2250, 2),4)+1), 
               lnCalcConcC=(log(rep(seq(0, 2250, 2),4)+1)) - 
                 mean(DataSum2496.centC$lnCalcConc) ,
               lnCalcConcC2=((log(rep(seq(0, 2250, 2),4)+1)) - 
                 mean(DataSum2496.centC$lnCalcConc))^2 ,               
               ExposeDummy = rep(c(0,1), each=1126*2), 
               SppDummy = rep(rep(c(0,1),each=1126),2), 
               RepID = rep(2,4504),
               npos = rep(mean(DataSum2496.centC$npos), 4504) )

# make zeros for individual random effects to predict to mean only -> model doesn't have any 'a' this way
  # a_fish_zeros = matrix(0,12000,90)
  # be_fish_zeros = matrix(0,12000,90)

# use estimated posteriors for population of individuals to simulate new fish; shapes of distributions between model posteriors and predicted posteriors are nearly identical, not what I want...but it does predict uncertainty across individuals, but might as well just take predictions for individual fish are put them in as the data...not quote grasping this
  # a_fish_sim = matrix(rnorm(12000*90, mean(post$a_fish), mean(post$sigma_fish[,1])),12000,90)
  # be_fish_sim = matrix(rnorm(12000*90, mean(post$be_fish), mean(post$sigma_fish[,2])),12000,90)

# replace individual fish estimates with global estimate; this should predict to the average individual in a population...so since I want to get at the population level response this seems more appropriate.
a_global = matrix(post$a,459000,90)
be_global = matrix(post$be,459000,90)

# use link to predict to mean only
# link.int2 <- link(centspp.blm_int2, n=1000, data= preddat, 
#                          replace = list(a_fish = a_global, 
#                                         be_fish = be_global) )
# link.int3 <- link(centspp.blm_int3, n=1000, data= preddat, 
#                          replace = list(a_fish = a_global, 
#                                         be_fish = be_global) )
# link.int4 <- link(centspp.blm_int4, n=1000, data= preddat, 
#                          replace = list(a_fish = a_global, 
#                                         be_fish = be_global) )
# link.int5 <- link(centspp.blm_int5, n=1000, data= preddat, 
#                          replace = list(a_fish = a_global, 
#                                         be_fish = be_global) )
# link.int6 <- link(centspp.blm_int6, n=1000, data= preddat, 
#                          replace = list(a_fish = a_global, 
#                                         be_fish = be_global) )
# link.int7 <- link(centspp.blm_int7, n=1000, data= preddat, 
#                          replace = list(a_fish = a_global, 
#                                         be_fish = be_global) )
link.int8 <- link(centspp.blm_int8, n=1000, data= preddat, 
                         replace = list(a_fish = a_global, 
                                        be_fish = be_global) )
preddat.df <- data.frame(sapply(preddat,c)) # thanks stack overflow! (turns the list into a dataframe) https://stackoverflow.com/questions/4227223/convert-a-list-to-a-data-frame

preddat.df$link_mu_mn <- apply(link.int8, 2, mean) 
preddat.df$link_PI05 = apply(link.int8, 2, PI, .95)[1,] 
preddat.df$link_PI95 = apply(link.int8, 2, PI, .95)[2,] 

# preddat.df$link_mu_mn <- apply(link.int5, 2, mean) 
# preddat.df$link_PI05 = apply(link.int5, 2, PI, .95)[1,] 
# preddat.df$link_PI95 = apply(link.int5, 2, PI, .95)[2,] 
 

# # predict from posteriors (on transformed scale)
#   link_pred <- link(distspp.blm_int2.rise, data = preddat, n=1000) 
#   preddat$link_mu_mn <- apply(link_pred, 2, mean)
#   preddat$link_PI05 = apply(link_pred, 2, PI, .95)[1,]
#   preddat$link_PI95 = apply(link_pred, 2, PI, .95)[2,]
#   
#  # predict with sim; it's my understanding that this integrates the error estimates too?
#   sim_pred <- sim(distspp.blm_int2.rise, data = preddat, n=1000) 
#   preddat$sim_mu_mn <- apply(sim_pred, 2, mean)
#   preddat$sim_PI05 = apply(sim_pred, 2, PI, .95)[1,]
#   preddat$sim_PI95 = apply(sim_pred, 2, PI, .95)[2,]
#  
  # plot predictions and data
  preddat.df$Spp <- ifelse(preddat.df$SppDummy==0,"GS","WS")
  preddat.df$ExposureHrs <- ifelse(preddat.df$ExposeDummy==0,"24","96")


CentPred_transf_plot2_mod8 = ggplot() + 
  geom_point(data=DataSum2496.centC,
             aes(x=(lnCalcConcC),                          
                 y=nCent/npos, 
                 color=factor(Spp))) + 
  geom_ribbon(data = preddat.df, aes(x=(lnCalcConcC), 
              ymin=link_PI05, ymax=link_PI95), 
              alpha=.4, col="grey70")+
  geom_line(data = preddat.df, aes(x=(lnCalcConcC), y=link_mu_mn), lwd=.2)+ 
    facet_grid(Spp~ExposureHrs, scales="free", labeller = labeller(
      ExposureHrs = c("24"="24 Hours Exposure","96"="96 Hours Exposure"),
      Spp = c("GS" = "Green Sturgeon", "WS" = "White Sturgeon")))+
    scale_color_manual(values=c("green3", "steelblue3"), guide=FALSE)+
    ylab("Proportion of Positions within Central Zone") + 
    scale_x_continuous(name="log(Bifenthrin Concentration) (ng/L)") + #, breaks = seq(-7.5,5, 2.5), labels=(seq(-7.5,5, 2.5)+3.73) ) +
    theme_bw()


CentPred_orig_plot2_mod8 = ggplot() + 
  geom_point(data=DataSum2496.centC,
             aes(x=(calcConc),                          
                 y=nCent/npos, 
                 color=factor(Spp))) + 
  geom_ribbon(data = preddat.df, aes(x=(CalcConc), 
              ymin=link_PI05, ymax=link_PI95), 
              alpha=.4, col="grey70")+
  geom_line(data = preddat.df, aes(x=(CalcConc), y=link_mu_mn), lwd=.2)+
  facet_grid(Spp~ExposureHrs, scales="free", labeller = labeller(
    ExposureHrs = c("24"="24 Hours Exposure","96"="96 Hours Exposure"),
    Spp = c("GS" = "Green Sturgeon", "WS" = "White Sturgeon")))+
  scale_color_manual(values=c("green3", "steelblue3"), guide=FALSE)+
  ylab("Proportion of Positions within Central Zone") + 
  scale_x_continuous(name="Bifenthrin Concentration (ng/L)") + #, breaks = seq(-7.5,5, 2.5), labels=(seq(-7.5,5, 2.5)+3.73) ) +
  theme_bw()


CentPred_orig_plot_96_mod8 = ggplot() + 
  geom_point(data=DataSum2496.centC[DataSum2496.centC$ExposureHrs==96,],
             aes(x=(calcConc),                          
                 y=nCent/npos, 
                 color=factor(Spp))) + 
  geom_ribbon(data = preddat.df[preddat.df$ExposureHrs==96,], 
              aes(x=(CalcConc), 
              ymin=link_PI05, ymax=link_PI95), 
              alpha=.4, col="grey70")+
  geom_line(data = preddat.df[preddat.df$ExposureHrs==96,],
            aes(x=(CalcConc), y=link_mu_mn), lwd=.2)+
  facet_grid(Spp~., scales="free", labeller = labeller(
    Spp = c("GS" = "Green Sturgeon", "WS" = "White Sturgeon")))+
  scale_color_manual(values=c("green3", "steelblue3"), guide=FALSE)+
  ylab("Proportion of Positions within Central Zone") + 
  scale_x_continuous(name="Bifenthrin Concentration (ng/L)") + #, breaks = seq(-7.5,5, 2.5), labels=(seq(-7.5,5, 2.5)+3.73) ) +
  theme_bw()


# CentPred_transf_plot2_mod1
# CentPred_transf_plot2_mod2
# CentPred_transf_plot2_mod3
# CentPred_transf_plot2_mod4
# CentPred_transf_plot2_mod5
# CentPred_transf_plot2_mod6
# CentPred_transf_plot2_mod7
CentPred_transf_plot2_mod8

# CentPred_orig_plot2_mod1
# CentPred_orig_plot2_mod2
# CentPred_orig_plot2_mod3
# CentPred_orig_plot2_mod4
# CentPred_orig_plot2_mod5
# CentPred_orig_plot2_mod6
# CentPred_orig_plot2_mod7
CentPred_orig_plot2_mod8
CentPred_orig_plot_96_mod8
```
 ### Hm. WAIC suggests they're nearly all equal, except for 7 (barely, nearly better). From looking at the fit of the prediction lines, I think 6 is not a good model because without the quadratic it doesn't fit the curve of the data and the mechanism we expected. Model 5 allows the quadratic to vary by species (green sturgeon curve becomes flatter in the transformed plots), and the effect of concentration to vary by exposure period (24 hr curve becomes flatter). This looks pretty good, but the confidence bands are wider than those for model 4 while the trend is very smiliar. Model 3 lets the quadratic vary by exposure period (but not spp), so I built/fit Model 7 to let both the standard and quadratic effectws for treatment vary by exposure period and species (ie: seperate line for each of the four facets). This model seems to be better, or at least not worse, based on AIC, and the plots look better too. Let's go with 7 - I think it's similar to the model for distance too, which will make the paper easier to write. Oops. Model 8 matches the distance model. And it is slightly better on AIC - it removes the interactions between treatment and exposure time. 
 
 
 
### Central Zone Contrasts

### Study questions: 

###  1) is there an effect (all treatment vs control), and which metrics show an effect
- posterior for bt x control vs posteriors for bt x each level of treatment (5 comps)
- in reality, this seems better nested under the following two contrast grouping, because the early-life stange behavior of the two species is so different, and it also seems that the WS are more affected (or perhaps just more mobile and this easier to detect change?)

###  2) Does the magnitude of the effect vary between species
- posterior for bt X GS x control vs posteriors for bt x GS x each level of treatment (5 comps)
- posterior for bt x WS x control vs posteriors for bt x WS x each level of treatment (5 comps)
= 10 comparisons

###  3) does that effect change with time [96 treat vs 96 control and 24 treat vs 24 control OR individual change from 24 to 96 (all treatments) as compared to individual change for control between 24 and 96]
- posterior for bt-24control vs bt-24xtreatments (n=5) and bt-96control vs bt-96 treatments (n=5) = 10 comparisons
- OR the above comparisions by species
= 20 comparisons. 


### Esh. Appendix table? mean difference, 95%CI or probability that the difference is not zero, which is more comparable to a p-value I think. It's the confidence that there IS a difference in the direction indicated (+ or -)



```{r cent multi-level model density plots and conc contrasts, echo=F}
set.seed(1983) 
postcent = extract.samples(centspp.blm_int8)
 
    # # reference table to convert from treatment to centered, log-transformed measured concentrations
TreatCalcTable = unique(DataSum2496.centC[order(DataSum2496.centC$Treatment)
                                             ,c("Spp", "Treatment", 
                                                "lnCalcConc", "lnCalcConcC")])
TreatCalcTable$lnCalcConcC2 = TreatCalcTable$lnCalcConcC ^ 2



# calculate differences between control and treatment at each time point for each species (20 contrasts)
GS0bif24cent = postcent$a +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==0)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==0)$lnCalcConcC2)
GS5bif24cent = postcent$a + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==5)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==5)$lnCalcConcC2)
GS100bif24cent = postcent$a + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==100)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==100)$lnCalcConcC2)
GS500bif24cent = postcent$a + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==500)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==500)$lnCalcConcC2)
GS1000bif24cent = postcent$a +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==1000)$lnCalcConcC) + 
  postcent$bt2* as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==1000)$lnCalcConcC2)
GS2000bif24cent = postcent$a + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==2000)$lnCalcConcC) + 
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==2000)$lnCalcConcC2)

# complile into a list
GS24cent.list = list(GS0bif24cent, GS5bif24cent, GS100bif24cent,
                     GS500bif24cent, GS1000bif24cent, GS2000bif24cent)

# write come output functions to reduce code length
logistic.summary.func = function(x) { mean = logistic(mean(x))
                                      Lower95CI = logistic(quantile(x,.025))
                                      Upper95CI = logistic(quantile(x,.975))
                                      return(data.frame(mean = mean,
                                                        U95CI = Upper95CI,
                                                        L95CI=Lower95CI)) }

# use functions to 
   pred.muGS24 = data.frame(nomconc = c(0,5,100,500,1000,2000), 
                            Spp = "GS",
                            ExposureHrs = 24,
                     pred.centZone = t(sapply(GS24cent.list,logistic.summary.func)),
                       diff.meancontrol = c(NA, sapply(GS24cent.list[2:6], function(x) mean(GS24cent.list[[1]]) - mean(x) )),
                     prob.greater0 = c(NA, mapply(function(x,y) sum(x<y), GS24cent.list[1], GS24cent.list[2:6]) / length(GS24cent.list[[1]]) ) )

         
 
GS24cent.plot = ggplot() + geom_density(aes(x=logistic(GS0bif24cent), color="black")) + 
  geom_density(aes(x=logistic(GS5bif24cent), color="green3")) + 
  geom_density(aes(x=logistic(GS100bif24cent), color="goldenrod")) + 
  geom_density(aes(x=logistic(GS500bif24cent), color="red3")) + 
  geom_density(aes(x=logistic(GS1000bif24cent), color="pink")) + 
  geom_density(aes(x=logistic(GS2000bif24cent), color="steelblue3")) + 
  scale_color_manual(name="Nominal Bifenthrin\nConcentration", 
                     values=c("black"="black", "green3"="green3", 
                              "goldenrod"="goldenrod","red3"="red3", 
                              "pink"="pink","steelblue3"="steelblue3"), 
                     labels=c("0 ng/L", "5 ng/L", "100ng/L", 
                              "500ng/L", "1000ng/L", "2000 ng/L") ) + 
  xlab("probability of Detection in Central Zone (GS @ 24hr)")+
  xlim(c(0,1))+
  theme_bw()







# repeat for green sturgeon, 96hr

GS0bif96cent = postcent$a + postcent$be +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==0)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==0)$lnCalcConcC2) 
GS5bif96cent = postcent$a + postcent$be + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==5)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==5)$lnCalcConcC2)
GS100bif96cent = postcent$a + postcent$be + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==100)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==100)$lnCalcConcC2)
GS500bif96cent = postcent$a + postcent$be + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==500)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==500)$lnCalcConcC2)
GS1000bif96cent = postcent$a + postcent$be +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==1000)$lnCalcConcC) + 
  postcent$bt2* as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==1000)$lnCalcConcC2)
GS2000bif96cent = postcent$a + postcent$be + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==2000)$lnCalcConcC) + 
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="GS" & Treatment==2000)$lnCalcConcC2)

## complile into a list
GS96cent.list = list(GS0bif96cent, GS5bif96cent, GS100bif96cent,
                     GS500bif96cent, GS1000bif96cent, GS2000bif96cent)

# write come output functions to reduce code length
logistic.summary.func = function(x) { mean = logistic(mean(x))
                                      Lower95CI = logistic(quantile(x,.025))
                                      Upper95CI = logistic(quantile(x,.975))
                                      return(data.frame(mean = mean,
                                                        U95CI = Upper95CI,
                                                        L95CI=Lower95CI)) }

# use functions to 
   pred.muGS96 = data.frame(nomconc = c(0,5,100,500,1000,2000), 
                            Spp = "GS",
                            ExposureHrs = 96,
                     pred.centZone = t(sapply(GS96cent.list,logistic.summary.func)),
                     diff.meancontrol = c(NA, sapply(GS96cent.list[2:6], function(x) mean(GS96cent.list[[1]]) - mean(x) )),
                     prob.greater0 = c(NA, mapply(function(x,y) sum(x<y), GS96cent.list[1], GS96cent.list[2:6]) / length(GS96cent.list[[1]]) ) )



GS96cent.plot = ggplot() + geom_density(aes(x=logistic(GS0bif96cent)), color="black") + 
  geom_density(aes(x=logistic(GS5bif96cent)), color="green3") + 
  geom_density(aes(x=logistic(GS100bif96cent)), color="goldenrod") + 
  geom_density(aes(x=logistic(GS500bif96cent)), color="red3") + 
  geom_density(aes(x=logistic(GS1000bif96cent)), color="pink") + 
  geom_density(aes(x=logistic(GS2000bif96cent)), color="steelblue3") + 
  scale_color_manual(name="Nominal Bifenthrin\nConcentration", 
                     values=c("black"="black", "green3"="green3", "goldenrod"="goldenrod",
                              "red3"="red3", "pink"="pink","steelblue3"="steelblue3"), 
                     labels=c("0 ng/L", "5 ng/L", "100ng/L", 
                              "500ng/L", "1000ng/L", "2000 ng/L") ) + 
  xlab("Proportion of Detections in Central Zone (GS @ 96hr)")+
  xlim(c(0,1))+
  theme_bw()


# repeat for white sturgeon, 24hr

WS0bif24cent = postcent$a + postcent$bs +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC2) +
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC2)
WS5bif24cent = postcent$a + postcent$bs + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC2)+
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC2)
WS100bif24cent = postcent$a + postcent$bs + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC2)+
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC2)
WS500bif24cent = postcent$a + postcent$bs + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC2)+
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC2)
WS1000bif24cent = postcent$a + postcent$bs +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC) + 
  postcent$bt2* as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC2)+
  postcent$bt2s* as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC2)
WS2000bif24cent = postcent$a + postcent$bs + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC) + 
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC2)+
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC2)



# complile into a list
WS24cent.list = list(WS0bif24cent, WS5bif24cent, WS100bif24cent,
                     WS500bif24cent, WS1000bif24cent, WS2000bif24cent)

# write come output functions to reduce code length
logistic.summary.func = function(x) { mean = logistic(mean(x))
                                      Lower95CI = logistic(quantile(x,.025))
                                      Upper95CI = logistic(quantile(x,.975))
                                      return(data.frame(mean = mean,
                                                        U95CI = Upper95CI,
                                                        L95CI=Lower95CI)) }

# use functions to 
   pred.muWS24 = data.frame(nomconc = c(0,5,100,500,1000,2000), 
                            Spp = "WS",
                            ExposureHrs = 24,
                     pred.centZone = t(sapply(WS24cent.list,logistic.summary.func)),
                     diff.meancontrol = c(NA, sapply(WS24cent.list[2:6], function(x) mean(WS24cent.list[[1]]) - mean(x) )),
                     prob.greater0 = c(NA, mapply(function(x,y) sum(x<y), WS24cent.list[1], WS24cent.list[2:6]) / length(WS24cent.list[[1]]) ) )

   
WS24cent.plot = ggplot() + geom_density(aes(x=logistic(WS0bif24cent)), color="black") + 
  geom_density(aes(x=logistic(WS5bif24cent)), color="green3") + 
  geom_density(aes(x=logistic(WS100bif24cent)), color="goldenrod") + 
  geom_density(aes(x=logistic(WS500bif24cent)), color="red3") + 
  geom_density(aes(x=logistic(WS1000bif24cent)), color="pink") + 
  geom_density(aes(x=logistic(WS2000bif24cent)), color="steelblue3") + 
  scale_color_manual(name="Nominal Bifenthrin\nConcentration", 
                     values=c("black"="black", "green3"="green3", "goldenrod"="goldenrod",
                              "red3"="red3", "pink"="pink","steelblue3"="steelblue3"), 
                     labels=c("0 ng/L", "5 ng/L", "100ng/L", 
                              "500ng/L", "1000ng/L", "2000 ng/L") ) + 
  xlab("Proportion of Detections in Central Zone (WS @ 24hr)")+
  xlim(c(0,1))+
  theme_bw()


# repeat for white sturgeon, 96hr


WS0bif96cent = postcent$a + postcent$bs + postcent$be + postcent$bse + 
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC2) +
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==0)$lnCalcConcC2)

WS5bif96cent = postcent$a + postcent$bs + postcent$be + postcent$bse +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC2) +
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==5)$lnCalcConcC2)

WS100bif96cent = postcent$a + postcent$bs + postcent$be + postcent$bse +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC2) +
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==100)$lnCalcConcC2)

WS500bif96cent = postcent$a + postcent$bs + postcent$be + postcent$bse +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC) +
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC2) +
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==500)$lnCalcConcC2)

WS1000bif96cent = postcent$a + postcent$bs + postcent$be + postcent$bse +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC) + 
  postcent$bt2* as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC2) +
  postcent$bt2s* as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==1000)$lnCalcConcC2)

WS2000bif96cent = postcent$a + postcent$bs + postcent$be + postcent$bse +
  postcent$bt*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC) + 
  postcent$bt2*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC2) +
  postcent$bt2s*as.numeric(subset(TreatCalcTable, Spp=="WS" & Treatment==2000)$lnCalcConcC2)


## complile into a list
WS96cent.list = list(WS0bif96cent, WS5bif96cent, WS100bif96cent,
                     WS500bif96cent, WS1000bif96cent, WS2000bif96cent)

# write come output functions to reduce code length
logistic.summary.func = function(x) { mean = logistic(mean(x))
                                      Lower95CI = logistic(quantile(x,.025))
                                      Upper95CI = logistic(quantile(x,.975))
                                      return(data.frame(mean = mean,
                                                        U95CI = Upper95CI,
                                                        L95CI=Lower95CI)) }

# use functions to 
   pred.muWS96 = data.frame(nomconc = c(0,5,100,500,1000,2000), 
                            Spp = "WS",
                            ExposureHrs = 96,
                     pred.centZone = t(sapply(WS96cent.list,logistic.summary.func)),
                     diff.meancontrol = c(NA, sapply(WS96cent.list[2:6], function(x) mean(WS96cent.list[[1]]) - mean(x) )),
                     prob.greater0 = c(NA, mapply(function(x,y) sum(x<y), WS96cent.list[1], WS96cent.list[2:6]) / length(WS96cent.list[[1]]) ) )


WS96cent.plot = ggplot() + geom_density(aes(x=logistic(WS0bif96cent)), color="black") + 
  geom_density(aes(x=logistic(WS5bif96cent)), color="green3") + 
  geom_density(aes(x=logistic(WS100bif96cent)), color="goldenrod") + 
  geom_density(aes(x=logistic(WS500bif96cent)), color="red3") + 
  geom_density(aes(x=logistic(WS1000bif96cent)), color="pink") + 
  geom_density(aes(x=logistic(WS2000bif96cent)), color="steelblue3") + 
  scale_color_manual(name="Nominal Bifenthrin\nConcentration", 
                     values=c("black"="black", "green3"="green3", "goldenrod"="goldenrod",
                              "red3"="red3", "pink"="pink","steelblue3"="steelblue3"), 
                     labels=c("0 ng/L", "5 ng/L", "100ng/L", 
                              "500ng/L", "1000ng/L", "2000 ng/L") ) + 
  xlab("Proportion of Detections in Central Zone (WS @ 96hr)")+
  xlim(c(0,1))+
  theme_bw()

GS24cent.plot + GS96cent.plot + WS24cent.plot + WS96cent.plot


# compile it for output
predgroups = list(GS0bif24cent, GS5bif24cent, GS100bif24cent,
                  GS500bif24cent, GS1000bif24cent, GS2000bif24cent,
                  GS0bif96cent, GS5bif96cent, GS100bif96cent,
                  GS500bif96cent, GS1000bif96cent, GS2000bif96cent,
                  WS0bif24cent, WS5bif24cent, WS100bif24cent,
                  WS500bif24cent, WS1000bif24cent, WS2000bif24cent,
                  WS0bif96cent, WS5bif96cent, WS100bif96cent,
                  WS500bif96cent, WS1000bif96cent, WS2000bif96cent)
PredGroupNames =  as.character(expression(GS0bif24cent, GS5bif24cent, GS100bif24cent, 
                                          GS500bif24cent, GS1000bif24cent, GS2000bif24cent, 
                                          GS0bif96cent, GS5bif96cent, GS100bif96cent, 
                                          GS500bif96cent, GS1000bif96cent, GS2000bif96cent,
                                          WS0bif24cent, WS5bif24cent, WS100bif24cent, 
                                          WS500bif24cent, WS1000bif24cent, WS2000bif24cent, 
                                          WS0bif96cent, WS5bif96cent, WS100bif96cent,
                                          WS500bif96cent, WS1000bif96cent, WS2000bif96cent))
posterior.preds = data.frame(PredGroupNames = PredGroupNames, 
                             PredMeans = sapply(predgroups, mean),
                             Pred95CIl = sapply(predgroups, quantile, 0.05),
                             Pred95CIu = sapply(predgroups, quantile, 0.95))

# try to compile again in a different way. Didn't know I'd already done this here, and did it again with each spp/exp group above. Oops. 
grouplist = list(pred.muGS24,pred.muGS96,pred.muWS24,pred.muWS96)
posterior.preds2 = do.call(rbind, grouplist)

```
#### if use this in a paper, recolor to a more intuitive gradient and add legend
 
 
```{r center zone  multi-level model time and spp contrasts, echo=F}
# this uses the predicted posteriors for time in central zone, calculated above

# GS vs WS
cont.GS.WS_0bif_24cent = sum(GS0bif24cent<WS0bif24cent) / length(GS0bif24cent)
 cont.GS.WS_0bif_24cent # 19.7% probability that GS will spend less time in center than WS, at 24hrs (ie: GS should be in the center more)

 cont.GS.WS_0bif_96cent = sum(GS0bif96cent<WS0bif96cent) / length(GS0bif96cent)
 cont.GS.WS_0bif_96cent # 42.7% probability that GS will spend less time in center than WS, at 96 hrs, which translates to a 57.2% probability that WS will spend less time in the center than GS. In otherwords, there's not  much difference in time spent in central zone at 96 hours because WS increase their use of the middle as they age
  
cont.GS.WS_5bif_24cent = sum(GS5bif24cent<WS5bif24cent) / length(GS5bif24cent)
 cont.GS.WS_5bif_24cent # 1.6% probability that GS will spend less time in center than WS, after 24 hrs exposure to 100ng/L bifenthrin (ie: GS found in center more than WS)

 cont.GS.WS_5bif_96cent = sum(GS5bif96cent<WS5bif96cent) / length(GS5bif96cent)
 cont.GS.WS_5bif_96cent # 6.2% probability that GS will spend less time in center than WS, after 96 hrs exposure to 100ng/L bifenthrin (ie: GS in center more than WS, but not quite to standard 5% probability 'significance' standard
   
cont.GS.WS_100bif_24cent = sum(GS100bif24cent<WS100bif24cent) / length(GS100bif24cent)
 cont.GS.WS_100bif_24cent # <0.001% probability that GS will spend less time in center than WS after 24hrs of exposure to 100ng/L bifenthrin. Ie: highly likely that GS spend more time in center than WS.
  
cont.GS.WS_100bif_96cent = sum(GS100bif96cent<WS100bif96cent) / length(GS100bif96cent)
 cont.GS.WS_100bif_96cent # 0.02% probability that GS will spend less time in center than WS after 24hrs of exposure to 100ng/L bifenthrin. Ie: highly likely that GS spend more time in center than WS.

 ## etc...
 
 cont.GS.WS_2000bif_96cent = sum(GS2000bif96cent<WS2000bif96cent) / length(GS2000bif96cent)
 cont.GS.WS_2000bif_96cent # 1.1% probability that GS will spend less time in center than WS at 96 hrs, when exposed to 2000ng/L bifenthrin. Ie: highly likely that GS spend more time in center than WS after 96hrs of exposure t0 2000ng/L bifenthrin
 
cont.GS.WS_2000bif_24cent = sum(GS2000bif24cent<WS2000bif24cent) / length(GS2000bif24cent)
 cont.GS.WS_2000bif_24cent # 0.2% probability that GS will spend less time in center than WS after 96hrs of exposure to 2000ng/L bifenthrin. Ie: highly likely that GS spend more time in center than WS.

 cont.GS.WS_2000bif_96cent = sum(GS2000bif96cent<WS2000bif96cent) / length(GS2000bif96cent)
 cont.GS.WS_2000bif_96cent # 0.01% probability that GS will spend less time in center than WS at 96 hrs, when exposed to 2000ng/L bifenthrin. Ie: highly likely that GS spend more time in center than WS after 96hrs of exposure t0 2000ng/L bifenthrin


# Green Sturgeon, Exposure Hr (no change at different levels since no exposure*treatment term in model
cont.GS0bif_96.24cent = (sum(GS0bif96cent<GS0bif24cent) / length(GS0bif96cent)) 
cont.GS0bif_96.24cent
 # 22.7% of the samples from the posterior showed less use of central zone by GS at 96 than 24; so most likely that central-zone use is greater at 96 than 24 (77.3%), but not 'significant' if using standard cut-off thresholds

GS0bif_96.24cent.plot = ggplot() + geom_density(aes(x=GS0bif96cent), color="steelblue3") + 
  geom_density(aes(x=GS0bif24cent), color="green3") + 
  xlab("Distance moved - GS @ 0 ng/L bifenthrin\n24 vs 96 hrs)") +
  theme_bw()

# use patchwork grammar to plot multiple plots
GS0bif_96.24cent.plot



# White Sturgeon: probability of a difference between 24 and 96 hours, at each treatment level
cont.WS0bif_96.24cent = (sum(WS0bif96cent<WS0bif24cent) / length(WS0bif96cent)) 
cont.WS0bif_96.24cent
# 2.1% of the samples from the posterior showed less use of central zone by WS at 96 than 24; so highly likely that WS use of central zone is greater at 96 than 24 (97.9%) 'significant' using standard thresholds

WS0bif_96.24cent.plot = ggplot() + geom_density(aes(x=WS0bif96cent), color="steelblue3") + 
  geom_density(aes(x=WS0bif24cent), color="green3") + 
  xlab("Distance moved - WS @ 0 ng/L bifenthrin\n24 vs 96 hrs)") +
  theme_bw()
WS0bif_96.24cent.plot


```
