---
title: "Exposure Ethovision Analysis - Data Cleaning"
author: "Anna Steel"
date: "2/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readtext) # need this because the ethovision output .txt files have unusual encoding as UTF-16
library(lme4)
library(rethinking)
library(circular)
library(rprojroot) # only used for find_rstudio_root_file() to set project wd as root
library(rstatix)

knitr::opts_knit$set(root.dir = find_rstudio_root_file()) # sets root directory to match project directory, not rmd file location
```

```{r utility functions}
# calculate hypotenuse of triangle created by two xy locations

distmov = function(x1,x2,y1,y2) { dm = sqrt((x1-x2)^2 + (y1-y2)^2); return(dm)}

# calculate number steps between detections; use summarized dataset (5 pos/sec)
nstep = function(t2,t1, interv=0.2) {
  if (t2-t1 < 0) stop('time steps backwards?')
  nstep <- ( (t2-t1) / interv)
  return(nstep)
  }

circ.mean.na = function(x) {
  x2 = x[!is.na(x)]
  sinr <- sum(sin(x2))
  cosr <- sum(cos(x2))
  circmean <- atan2(sinr, cosr)
  return(circmean)
}


circ.disp.na = function(x) {
  x2 = x[!is.na(x)]
    n <- length(x2)
    c <- sum(cos(x2))
    s <- sum(sin(x2))
    r <- sqrt(c^2 + s^2)
    rbar <- r/n
    var <- 1 - rbar
    data.frame(n, r, rbar, var)
}

```


## Prep Data for 2022 WS Exposures
  
The data used here were tracked in Ethovision XT17 then filtered with "Lowess 5 points, Max 5cm, Min 0.2cm". As of Oct 2022 no additional editing has been completed
  
#### Read in metadata 
```{r read metadata}
# set directory where .txt files are for each species
WSdatadir = "/Users/Anna/Documents/ResearchGit/SturgContam/rawData/Exposure_EthoExport_Complete_WS2022/"
  
# read in file names 
WSfilelist = list.files(path=WSdatadir, pattern="Track-WScontam") 
  ## for now I've pulled out everything tha isn't 96hrs
```

#### Read in data
```{r read data}
# prep new column names for datafiles (current ones buried in .txt header)
    datcolNamesWS = c("TrialTime_s","RecTime_s",
                      "Xcent_mm","Ycent_mm","Xnose_mm","Ynose_mm","Xtail_mm","Ytail_mm",
                    "Area_mm2","AreaChange_mm2","Elong",
                    "Direc_deg","TurnAngleAbs_deg","AngVelAbs_degS","MeanderAbs_degMM",
                    "InZone_Cent25","InZone_Cent15",
                    "DistMoved_mm","X1")
 
# function to read and ammend ethovision files to compile later     
 readTrialDat.func =function(datadir, datcolNames, x) {
      trialidat = read.csv(paste0(datadir, x), sep=",", 
                           skip=43, header=F, fileEncoding="UTF-16")
      trialheader = read.csv(paste0(datadir, x), sep=",",
                             header=F, fileEncoding="UTF-16")[1:43,]
      names(trialidat) = datcolNames
      trialidat$X1 <-NULL
      
      trialidat$temp <- trialheader[37,2]
      trialidat$nomconc_ngL <- trialheader[39,2]
      trialidat$ExposureHrs <- trialheader[35,2]
      trialidat$tray <- trialheader[38,2]
      trialidat$rep <- trialheader[40,2]
      trialidat$SubjectNotFound <- trialheader[30,2]
      trialidat$TrackLength <- trialheader[16,2]
      trialidat$TrialDateTime <- trialheader[20,2]
      trialidat$Trial <- trialheader[4,2]
      trialidat$Arena <- trialheader[6,2]
      
    Treatment = paste0(trialidat$ExposureHrs[1], "-", trialidat$temp[1],"C-", 
                       trialidat$nomconc[1],"ngL-R", trialidat$rep[1])
    print(Treatment) 
      # check point to make sure it's working correctly; can comment out if it is
      
      return(trialidat) 
    }
    
    
# do.call function to use above function to read and then rbind all the trial datafiles into one df per species    

  WSdata = do.call(rbind, lapply(WSfilelist, readTrialDat.func, datadir = WSdatadir,  datcolNames = datcolNamesWS) )

```

```{r corrections}
# remove trial 45 until can edit later
WSdata = WSdata[WSdata$tray!="C",]
```

#### summarize the data into larger time-chunks to facilitate analysis and visualization (the large dataset doesn't move particularly fast...). Recorded with 30 positions per second, summarized those to 5 positions per second (each trial thus created 6000 positions over 20 minutes, if all data were collected).    

#### Also add datacolumn for distanced moved where NA is replaced with 0 

```{r cut data}

WSdatasub = WSdata[,c("Arena","rep","ExposureHrs","nomconc_ngL",
                      "tray","temp","TrackLength","TrialTime_s","RecTime_s",
                      "Xcent_mm","Ycent_mm","Xnose_mm","Ynose_mm","Xtail_mm","Ytail_mm",
                    "Area_mm2","AreaChange_mm2","Elong",
                    "Direc_deg","TurnAngleAbs_deg","AngVelAbs_degS","MeanderAbs_degMM",
                    "InZone_Cent25","InZone_Cent15",
                    "DistMoved_mm")]

WSdatasub$cut = cut_width(WSdatasub$RecTime_s, 0.2, boundary=0, labels=F)

WSdatasum = WSdatasub %>%
  group_by(temp,ExposureHrs,nomconc_ngL,Arena,rep,tray,cut) %>%
  summarize(TrialTime = round(mean(RecTime_s),1),
            Xcent = mean(Xcent_mm, na.rm=T),
            Ycent = mean(Ycent_mm, na.rm=T),
            MnDistMoved = mean(DistMoved_mm, na.rm=T),
            SumDistMoved = sum(DistMoved_mm, na.rm=T),
            #Meander = mean(Meander_degmm, na.rm=T),
           # TurnAngleCirc = deg(circ.mean.na(rad(TurnAngleRel_deg2))),
            InZone_Cent25 = max(InZone_Cent25),
            InZone_Cent15 = max(InZone_Cent15)) %>%
  ungroup() %>%
  data.frame()

# rather than summarize the meander at each step, recalculate it at these summarized steps by taking the mean heading (turn angle over the 0.2 secs) and divide it by the total distance moved in that same period, following the calculations made by Ethovision to calculate meander. 
#WSdatasum$MeanderCirc = WSdatasum$TurnAngleCirc/WSdatasum$SumDistMoved

```

#### combine WS and GS data
```{r comb data}
DataSum = WSdatasum
 #DataSum$Trial = factor(DataSum$Trial) 
 DataSum$rep = factor(DataSum$rep) 
 #DataSum$RepID = as.factor(paste0(DataSum$Treatment, "-", DataSum$Spp, "-", DataSum$Replicate))
 
 DataSum$uniqueID = paste(DataSum$nomconc_ngL, DataSum$temp, DataSum$rep, sep="-") 
```

```{r reduce dataset further} 
# # reduce range of data to analyze; 3min of acclimation, 12 min of trial (180 - 900 s, should have 3600 data points per fish per trial). That was used for 2020, so we'll use it here too for comparison. Can toy with adding more trial time but don't think it's necessary
#  
# DataSumcut15 = DataSum[DataSum$TrialTime<(14*60),] # the first tray didn't have lights ona t beginning of trial, so the trial time and the record time don't match the rest of the dataset. Thus, I've switched the above to 'rectime_s' instead, which begins 60seconds into the official trial (after lights are on and test conditions are met). Thus, this is 14*60 becase that is 15 minutes into the trial, 14 minutes into recording. 
# 
# # even smaller dataset to plot only the exposure hours that are comparable among the two species
# DataSum2496nom = DataSumcut15[DataSumcut15$ExposureHrs %in% c(24,96),]
# 
# # add unique line id to this reduced dataset
# DataSum2496nom$lineID = 1:nrow(DataSum2496nom)
# 
# # keep full dataset with all four video recordings
# DataSumAllnom = DataSumcut15
# DataSumAllnom$lineID = 1:nrow(DataSumAllnom)

```


#### add measured concentrations (versus nominal)
```{r chem analysis results}
  # # don't have this for 2021 yet; should I / can I still present the data next week?
  # chemcsv = read.csv("/Users/Anna/Documents/ResearchGit/SturgContam/rawData/ChemAnalysis_Summary.csv")
  #   chemcsv <- chemcsv[, c("spp", "nomconc", "calcConc")]
  #  
  # DataSum2496 <- merge(DataSum2496nom, chemcsv, by.x=c("Spp","Treatment"), by.y=c("spp","nomconc"), all.x=T)
  # 
  # DataSumAll <- merge(DataSumAllnom, chemcsv, by.x=c("Spp","Treatment"), by.y=c("spp","nomconc"), all.x=T)


# until I get those chem concentration values:
DataSum2496 <- DataSum2496nom
DataSumAll <- DataSumAllnom
```



#### write out cleaned dataframe for use in subsequent codes
```{r write out data, echo=FALSE, eval=FALSE}

write.csv(DataSum, "outputData/Exposure_Outputdata/Bifenthrin_2022_Cleaned_NoConc_96hrs.csv", row.names=F)

```
























Old Code:

Insert ~ ln200

#### interpolate missing datapoints - testing testing fail
```{r interpolate attempt1, eval=FALSE, include=FALSE}
# about 6% of the timepoints in the reduced dataset are missing data at the MnDistMoved column (41,006 out of 669,600 total) because of removed data points in filtering
# of those the vast majority are GS (40173 of 41006) because of the poorer recording conditions

# here I remove all rows without X and Y centroids, and use my own function to calculate distance moved between the summarized centroids. I can't use this abbreviated dataset 

DataSum2496nom_dist = DataSum2496nom %>%
  group_by(uniqueID) %>%
  subset(!is.na(MnDistMoved)) %>%
  mutate(SumDistMoved.Abbrev = distmov(x1=lag(Xcent), x2=Xcent, y1=lag(Ycent), y2=Ycent) ) %>%
  ungroup() %>%
  data.frame()
           
# merge it back into the interpolated data 
DS = merge(DataSum2496nom, DataSum2496nom_dist[, c("lineID","SumDistMoved.Abbrev")], by="lineID", all.x=T)



# look at point differences when use these two methods; does it vary by species, treatment, or time?
ggplot(DS, aes(x=factor(Treatment), y=(SumDistMoved - SumDistMoved.Abbrev) ) ) + 
         geom_point() + geom_hline(yintercept=0, col="steelblue3") + 
         facet_grid(ExposureHrs~Spp) + 
         ylab("Difference in Distance Moved\nCalculated at 30fps vs 4fps centroids") + 
         theme_bw()
# removed 41,192 rows without data

# A negative number indicates that the distance calculated from 5fps centroids is greater than the distance calculated from 30fps. This makes sense because if there is a missing point between two detections, the distance between those detections isn't included in the total estimate of distance by ethovision but with my code it is included and assumed to be a straight line (underestimate). Additionall;y, with this code any erroneous points that weren't filtered out successfully (likely because they didn't cause a distance to be recorded in the ethovision track, but the points were still present), the erroneous points are included in the interpolation and additional ines are drawn from the fish tothe cross-dish reflection and back. Argh. However, sometimes the 5fps is smaller than 30 fps (positive point) because there was tortuosity in the path that was eliminated during abbreviation/summarization step going from 20-5fps. 

# it seems as though the difference is larger for GS, which makes sense if it's linked to poor detections, extra manual filtering, and both missing and erroneous points. 


# look at differences in total distance for each trial 
DS.totaldist = DS %>%
  group_by(Spp, Treatment, ExposureHrs, Replicate, Arena) %>%
  summarize(total.dist.30fps = sum(SumDistMoved, na.rm=T), 
            total.dist.5fps = sum(SumDistMoved.Abbrev, na.rm=T)) %>%
  data.frame()

ggplot(subset(DS.totaldist, Spp=="GS"), aes(x=Replicate, y=total.dist.30fps)) + 
  geom_point(pch=16, color="green3") + geom_point(aes(y=total.dist.5fps), pch=16, color="steelblue3") +
  facet_grid(ExposureHrs ~ Treatment)

ggplot(subset(DS.totaldist, Spp=="WS"), aes(x=Replicate, y=total.dist.30fps)) + 
  geom_point(pch=17, color="green3") + geom_point(aes(y=total.dist.5fps), pch=16, color="steelblue3") +
  facet_grid(ExposureHrs ~ Treatment)

# 30fps is always longer total distance than 5fps but the trends seem consistent and there doesn't seem to be an obvious visual bias by treatment

DS.distsd = DS.totaldist %>%
  group_by(Spp, Treatment, ExposureHrs) %>%
  summarize(sd.30fps = sd(total.dist.30fps, na.rm=T), 
          sd.5fps = sd(total.dist.5fps, na.rm=T)) %>%
  ungroup() %>%
  data.frame()

ggplot(DS.distsd, aes(x=factor(Treatment), y=sd.30fps)) + 
  geom_point(pch=17, color="green3") + geom_point(aes(y=sd.5fps), pch=16, color="steelblue3") +
  facet_grid(Spp ~ ExposureHrs) 
# variance among trials is simliar for GS but slightly lower at 5fps, but for WS the trend isn't as clear. At 24hrs, the variance is higher with 5fps uexcept for the 2000 exposure, and at 96hrs its all over the board, but generally the 30fps is higher (100, 1000, 2000), but sometimes lower (1000) and sometimes the same (0, 5).  

```

```{r interpolation notes only, eval=FALSE, include=FALSE}

## To summarize (8/18/2021) I've attempted to explore the usefulness of calculating distance moved from the 30fps dataset in ethovision, or calculating distance moved from the 5fps dataset after it has been condensed/sumarized. In the step for re-calculating distance after the fact, I've also used all sequential locations and assumed linear paths between them, even when there were substantial temporal gaps between detections. The results from this do not lead in a clear direction for continued analysis. The options are:   
#1) use Ethovision's estimated distance, and de-facto assume the fish is still when the track is lost (often true but not always)  
#2) use distances calculated here, at 5 fps, but this feels unsatisfying because it is clear that this does lose some tortuosity (unclear whether that is real or erronious, but if we're trusting the software and post-processing then we must assume it's largely real).  Also, upon a second review of the editing process, there were occational erroneous points left in as long as they didn't create long-distance movements. So with the ethovision distance estimate, erroneous points should not have a large effect. However, if I interpolate between all existing points then I'll be adding these long-distance erroneous movement to/from the reflection point BACK into the distance estimate. This is not correct, and likely more of a problem then the assumption that missing data = stationary fish. So this choice isn't preferable. 
#3) re-jigger this code to calculate post-hoc distance moved for 30fps but for all sequential detections. This seems the most satisfying because it won't lose the tortuosity of the track but it will allow us to estimate an distance moved when the software loses a moving fish (if it is stationary it will also account for that). However, again, there is the3 issue with the poor detection/reflection and the limited editing. I think the editing is fine for if I use the ethovision distances, but maybe problematic if I interpolate. 

#- so after all this days work, I think perhaps we go with the data we have, fill in the zeros for moving/not moving, fill in the zone ID for the last known location, and start the analysis. 
#- Analysis will include four output variables for effects of treatment and time for each species: distance = activity index; % time active = activity index; turning or meander = directionality (body control?), center zone = thigmotaxis and boldness). Two four-panel plots to show results (raw and predicted? just raw?). 
```
